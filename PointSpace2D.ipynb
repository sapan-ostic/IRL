{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.0\n"
     ]
    }
   ],
   "source": [
    "# ! rm -r runs\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "from collections import namedtuple\n",
    "\n",
    "import gym\n",
    "import gym_point\n",
    "import numpy as np\n",
    "import ptan\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "import torch.optim as optim\n",
    "from ptan.agent import float32_preprocessor\n",
    "\n",
    "from util import PGN, RewardNet\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.001\n",
    "EPISODES_TO_TRAIN = 4\n",
    "DEMO_BATCH = 100\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['state', 'action', 'reward', 'next_state'])\n",
    "Trajectory = namedtuple('Trajectory', field_names=['prob', 'episode_steps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_qvals(rewards):\n",
    "    res = []\n",
    "    sum_r = 0.0\n",
    "    for r in reversed(rewards):\n",
    "        sum_r *= GAMMA\n",
    "        sum_r += r\n",
    "        res.append(sum_r)\n",
    "    return list(reversed(res))\n",
    "\n",
    "\n",
    "# def process_demonstrations(demo_samples):\n",
    "#     traj_states, traj_actions, traj_qvals, traj_prob = [], [], [], []\n",
    "#     for traj in demo_samples:\n",
    "#         states, actions, rewards, qvals = [], [], [], []\n",
    "#         traj_prob.append(traj.prob)\n",
    "#         for step in traj.episode_steps:\n",
    "#             states.append(step.state)\n",
    "#             actions.append(step.action)\n",
    "#             rewards.append(step.reward)\n",
    "#         qvals.extend(calc_qvals(rewards))\n",
    "\n",
    "#         traj_states.append(states)\n",
    "#         traj_actions.append(actions)\n",
    "#         traj_qvals.append(qvals)\n",
    "#     traj_states = np.array(traj_states, dtype=np.object)\n",
    "#     traj_actions = np.array(traj_actions, dtype=np.object)\n",
    "#     traj_qvals = np.array(traj_qvals, dtype=np.object)\n",
    "#     traj_prob = np.array(traj_prob, dtype=np.float)\n",
    "#     return {'states': traj_states, 'actions': traj_actions, 'qvals': traj_qvals, 'traj_probs': traj_prob}\n",
    "\n",
    "def process_demonstrations(demo_samples):\n",
    "    traj_states, traj_actions = [], []\n",
    "    for traj in demo_samples:\n",
    "        states, actions = [], []\n",
    "\n",
    "        for step in traj.episode_steps:\n",
    "            states.append(step.state)\n",
    "            actions.append(step.action)\n",
    "\n",
    "        traj_states.append(states)\n",
    "        traj_actions.append(actions)\n",
    "    traj_states = np.array(traj_states, dtype=np.object)\n",
    "    traj_actions = np.array(traj_actions, dtype=np.object)\n",
    "    return {'states': traj_states, 'actions': traj_actions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagrawal/src/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: The dimensions are bigger than 2, only the first 2 dimensions are visualized\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('PointContinuousEnv-v0')\n",
    "agent_net = PGN(env.observation_space.shape[0], env.action_space.n)\n",
    "reward_net = RewardNet(env.observation_space.shape[0] + 1)\n",
    "# reward_net = RewardNet(2)\n",
    "agent = ptan.agent.PolicyAgent(agent_net, preprocessor=float32_preprocessor, apply_softmax=True)\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=GAMMA)\n",
    "optimizer_agent = optim.Adam(agent_net.parameters(), lr=LEARNING_RATE)\n",
    "optimizer_reward = optim.Adam(reward_net.parameters(), lr=1e-2, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Generate Expert Demonstrations \n",
    "# x = [x, y, xdot, ydot]\n",
    "# xdot = [xdot, ydot, xddot, yddot]\n",
    "# Plotting variables\n",
    "\n",
    "Xdes = []\n",
    "Ydes = []\n",
    "Acc = []\n",
    "        \n",
    "def control(s, timer, dt=0.01):\n",
    "    Kp = -100   # Controller gains \n",
    "    Kd = -3\n",
    "    \n",
    "    xdes = timer + dt\n",
    "    Xdes.append(xdes)\n",
    "    \n",
    "    amp_noise = np.random.normal(0, 0.001, 1)\n",
    "    x_noise = np.random.normal(0, 0.1, 1)\n",
    "    freq_noise = np.random.uniform(0.95, 1.1, 1)\n",
    "    \n",
    "    noise = np.random.uniform(-0.01,0.01)\n",
    "#     ydes = -0.5*np.sin(-6.28*(timer+dt)/freq_noise + x_noise) + amp_noise\n",
    "    ydes = -0.5*np.sin(-6.28*(timer+dt))\n",
    "    Ydes.append(ydes)\n",
    "    ex = s[0] - xdes\n",
    "    ey = s[1] - ydes\n",
    "    ex_dot = s[2]\n",
    "    ey_dot = s[3]\n",
    "    ax = np.array(np.round(Kp*ex + Kd*ex_dot))\n",
    "    ay = np.array(np.round(Kp*ey + Kd*ey_dot))\n",
    "    np.clip(ax, -1, 1, out=ax)\n",
    "    np.clip(ay, -1, 1, out=ay)\n",
    "    return (ax, ay) \n",
    "\n",
    "def encode_action(ax, ay):\n",
    "    if ax == -1 and ay == -1:\n",
    "        action = 0\n",
    "    elif ax == -1 and ay == 0:\n",
    "        action = 1\n",
    "    elif ax == -1 and ay == 1:\n",
    "        action = 2\n",
    "    elif ax == 0 and ay == -1:\n",
    "        action = 3\n",
    "    elif ax == 0 and ay == 0:\n",
    "        action = 4\n",
    "    elif ax == 0 and ay == 1:\n",
    "        action = 5\n",
    "    elif ax == 1 and ay == -1:\n",
    "        action = 6\n",
    "    elif ax == 1 and ay == 0:\n",
    "        action = 7\n",
    "    elif ax == 1 and ay == 1:\n",
    "        action = 8\n",
    "    return action\n",
    "\n",
    "def get_demonstrations(Ndemo=50):\n",
    "    dt = 0.01  # Discrete time step\n",
    "    demo_states = []\n",
    "    demo_actions = []\n",
    "\n",
    "    for idemo in range(Ndemo):\n",
    "        state = env.reset()\n",
    "\n",
    "        # Storage variables\n",
    "        XStore = [state.copy()] # State trajectory\n",
    "        AStore = [] # initial action 1, 1 # Action trajectory\n",
    "        RStore = [] # Reward trajectory\n",
    "        TStore = []  # Time trajectory\n",
    "        AxStore = []\n",
    "        AyStore = []\n",
    "        # Initializations\n",
    "        done = False \n",
    "        i = 1\n",
    "        timer = 0\n",
    "\n",
    "        while not done:\n",
    "            timer = i*dt\n",
    "            state[1] += np.random.normal(0, 0.03, 1)\n",
    "            ax, ay = control(state, timer)\n",
    "            action = encode_action(ax, ay)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "                            \n",
    "            state = next_state.copy()\n",
    "            \n",
    "            # Store variables\n",
    "            XStore.append(state.copy())\n",
    "            AStore.append(action)\n",
    "            AxStore.append(ax)\n",
    "            AyStore.append(ay)\n",
    "            RStore.append(reward)\n",
    "            TStore.append(timer)\n",
    "            Acc.append([ax,ay])\n",
    "    \n",
    "            i += 1\n",
    "            \n",
    "        if info == 'out_of_margin':\n",
    "            continue\n",
    "\n",
    "        # sample = [XStore, AStore, RStore]\n",
    "        demo_states.append(XStore[:-1])\n",
    "        demo_actions.append(AStore)\n",
    "        # demo_trajs.append(sample)\n",
    "        plt.plot(np.array(XStore)[:,0], np.array(XStore)[:,1], '#aed6f1')\n",
    "#         plt.plot(np.array(XStore)[:,0], np.array(XStore)[:,1])\n",
    "#         plt.plot(np.array(Xdes), np.array(Ydes))\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('y')\n",
    "        plt.title('Demonstrations')\n",
    "        \n",
    "#         plt.figure()\n",
    "#         plt.plot(np.array(TStore), np.array(AxStore[:-1]))\n",
    "#         plt.xlabel('t')\n",
    "#         plt.ylabel('ax')\n",
    "        \n",
    "#         plt.figure()\n",
    "#         plt.plot(np.array(TStore), np.array(AyStore[:-1]))\n",
    "#         plt.xlabel('t')\n",
    "#         plt.ylabel('ay')\n",
    "        \n",
    "    # demo_trajs = np.asarray(demo_trajs)\n",
    "    # print(\"Total Samples: \", len(demo_trajs))\n",
    "    # print(\"Demonstrations Shape: \", demo_trajs.shape, \"[no. of samples, {State, Action, Reward}, No. of points]\")\n",
    "    keys = ['states', 'actions']\n",
    "    demonstrations = dict.fromkeys(keys, None)\n",
    "    demonstrations['states'] = demo_states\n",
    "    demonstrations['actions'] = demo_actions\n",
    "    return demonstrations\n",
    "\n",
    "demonstrations = get_demonstrations(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test case:  0  Reward:  100\n",
      "Test case:  50  Reward:  100\n"
     ]
    }
   ],
   "source": [
    "## Testing Demonstration\n",
    "\n",
    "def test_demonstration(demonstrations):\n",
    "    for i in range(0,100, 50):\n",
    "        state = env.reset()\n",
    "        Reward = 0\n",
    "        done = False\n",
    "        j = 0\n",
    "        while not done:\n",
    "            env.render()\n",
    "            action = demonstrations['actions'][i][j]\n",
    "            new_state, reward, done, _ = env.step(int(action))\n",
    "            Reward += reward \n",
    "            j += 1 \n",
    "        print(\"Test case: \", i,  ' Reward: ', Reward)\n",
    "\n",
    "test_demonstration(demonstrations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('demonstrations.list.pkl', 'rb') as f:\n",
    "#     demonstrations = pickle.load(f)\n",
    "# assert (len(demonstrations) > DEMO_BATCH)\n",
    "# print(f'Number of demonstrations: {len(demonstrations)}')\n",
    "# demonstrations = process_demonstrations(demonstrations) \n",
    "# dict_keys(['states', 'actions', 'qvals', 'traj_probs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# States[100 x 500 x 4]\n",
    "# Actions[100 x 500 x 1]\n",
    "# demonstrations['states'] # Array of lists of actions\n",
    "# demonstrations['actions'] # Array of lists of array of states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100: reward: -92.00, mean_100: -92.00, episodes: 1, reward function loss: 0.0000\n",
      "200: reward: -88.00, mean_100: -90.00, episodes: 2, reward function loss: 0.0000\n",
      "300: reward: -92.00, mean_100: -90.67, episodes: 3, reward function loss: 0.0000\n",
      "400: reward: -88.00, mean_100: -90.00, episodes: 4, reward function loss: -192.8937\n",
      "500: reward: -68.00, mean_100: -85.60, episodes: 5, reward function loss: -192.8937\n",
      "600: reward: -78.00, mean_100: -84.33, episodes: 6, reward function loss: -192.8937\n",
      "700: reward: -88.00, mean_100: -84.86, episodes: 7, reward function loss: -192.8937\n",
      "800: reward: -92.00, mean_100: -85.75, episodes: 8, reward function loss: -186.3432\n",
      "900: reward: -90.00, mean_100: -86.22, episodes: 9, reward function loss: -186.3432\n",
      "1000: reward: -92.00, mean_100: -86.80, episodes: 10, reward function loss: -186.3432\n",
      "1100: reward: -92.00, mean_100: -87.27, episodes: 11, reward function loss: -186.3432\n",
      "1200: reward: -94.00, mean_100: -87.83, episodes: 12, reward function loss: -183.8624\n",
      "1300: reward: -84.00, mean_100: -87.54, episodes: 13, reward function loss: -183.8624\n",
      "1400: reward: -92.00, mean_100: -87.86, episodes: 14, reward function loss: -183.8624\n",
      "1500: reward: -90.00, mean_100: -88.00, episodes: 15, reward function loss: -183.8624\n",
      "1600: reward: -66.00, mean_100: -86.62, episodes: 16, reward function loss: -183.4154\n",
      "1700: reward: -90.00, mean_100: -86.82, episodes: 17, reward function loss: -183.4154\n",
      "1800: reward: -86.00, mean_100: -86.78, episodes: 18, reward function loss: -183.4154\n",
      "1900: reward: -88.00, mean_100: -86.84, episodes: 19, reward function loss: -183.4154\n",
      "2000: reward: -94.00, mean_100: -87.20, episodes: 20, reward function loss: -236.7538\n",
      "2100: reward: -86.00, mean_100: -87.14, episodes: 21, reward function loss: -236.7538\n",
      "2200: reward: -90.00, mean_100: -87.27, episodes: 22, reward function loss: -236.7538\n",
      "2300: reward: -84.00, mean_100: -87.13, episodes: 23, reward function loss: -236.7538\n",
      "2400: reward: -90.00, mean_100: -87.25, episodes: 24, reward function loss: -245.6373\n",
      "2500: reward: -92.00, mean_100: -87.44, episodes: 25, reward function loss: -245.6373\n",
      "2600: reward: -82.00, mean_100: -87.23, episodes: 26, reward function loss: -245.6373\n",
      "2700: reward: -90.00, mean_100: -87.33, episodes: 27, reward function loss: -245.6373\n",
      "2800: reward: -92.00, mean_100: -87.50, episodes: 28, reward function loss: -246.0635\n",
      "2900: reward: -92.00, mean_100: -87.66, episodes: 29, reward function loss: -246.0635\n",
      "3000: reward: -92.00, mean_100: -87.80, episodes: 30, reward function loss: -246.0635\n",
      "3100: reward: -92.00, mean_100: -87.94, episodes: 31, reward function loss: -246.0635\n",
      "3200: reward: -90.00, mean_100: -88.00, episodes: 32, reward function loss: -246.1171\n",
      "3300: reward: -68.00, mean_100: -87.39, episodes: 33, reward function loss: -246.1171\n",
      "3400: reward: -92.00, mean_100: -87.53, episodes: 34, reward function loss: -246.1171\n",
      "3500: reward: -84.00, mean_100: -87.43, episodes: 35, reward function loss: -246.1171\n",
      "3600: reward: -90.00, mean_100: -87.50, episodes: 36, reward function loss: -243.4146\n",
      "3700: reward: -88.00, mean_100: -87.51, episodes: 37, reward function loss: -243.4146\n",
      "3800: reward: -92.00, mean_100: -87.63, episodes: 38, reward function loss: -243.4146\n",
      "3900: reward: -68.00, mean_100: -87.13, episodes: 39, reward function loss: -243.4146\n",
      "4000: reward: -76.00, mean_100: -86.85, episodes: 40, reward function loss: -245.5533\n",
      "4100: reward: -80.00, mean_100: -86.68, episodes: 41, reward function loss: -245.5533\n",
      "4200: reward: -88.00, mean_100: -86.71, episodes: 42, reward function loss: -245.5533\n",
      "4300: reward: -84.00, mean_100: -86.65, episodes: 43, reward function loss: -245.5533\n",
      "4400: reward: -86.00, mean_100: -86.64, episodes: 44, reward function loss: -183.9894\n",
      "4500: reward: -90.00, mean_100: -86.71, episodes: 45, reward function loss: -183.9894\n",
      "4600: reward: -92.00, mean_100: -86.83, episodes: 46, reward function loss: -183.9894\n",
      "4700: reward: -88.00, mean_100: -86.85, episodes: 47, reward function loss: -183.9894\n",
      "4800: reward: -94.00, mean_100: -87.00, episodes: 48, reward function loss: -246.1177\n",
      "4900: reward: -50.00, mean_100: -86.24, episodes: 49, reward function loss: -246.1177\n",
      "5000: reward: -90.00, mean_100: -86.32, episodes: 50, reward function loss: -246.1177\n",
      "5100: reward: -90.00, mean_100: -86.39, episodes: 51, reward function loss: -246.1177\n",
      "5200: reward: -44.00, mean_100: -85.58, episodes: 52, reward function loss: -246.0676\n",
      "5300: reward: -64.00, mean_100: -85.17, episodes: 53, reward function loss: -246.0676\n",
      "5400: reward: -80.00, mean_100: -85.07, episodes: 54, reward function loss: -246.0676\n",
      "5500: reward: -92.00, mean_100: -85.20, episodes: 55, reward function loss: -246.0676\n",
      "5600: reward: -56.00, mean_100: -84.68, episodes: 56, reward function loss: -245.9449\n",
      "5700: reward: -86.00, mean_100: -84.70, episodes: 57, reward function loss: -245.9449\n",
      "5800: reward: -92.00, mean_100: -84.83, episodes: 58, reward function loss: -245.9449\n",
      "5900: reward: -92.00, mean_100: -84.95, episodes: 59, reward function loss: -245.9449\n",
      "6000: reward: -92.00, mean_100: -85.07, episodes: 60, reward function loss: -246.0545\n",
      "6100: reward: -92.00, mean_100: -85.18, episodes: 61, reward function loss: -246.0545\n",
      "6200: reward: -88.00, mean_100: -85.23, episodes: 62, reward function loss: -246.0545\n",
      "6300: reward: -90.00, mean_100: -85.30, episodes: 63, reward function loss: -246.0545\n",
      "6400: reward: -90.00, mean_100: -85.38, episodes: 64, reward function loss: -246.1129\n",
      "6500: reward: -80.00, mean_100: -85.29, episodes: 65, reward function loss: -246.1129\n",
      "6600: reward: -90.00, mean_100: -85.36, episodes: 66, reward function loss: -246.1129\n",
      "6700: reward: -94.00, mean_100: -85.49, episodes: 67, reward function loss: -246.1129\n",
      "6800: reward: -74.00, mean_100: -85.32, episodes: 68, reward function loss: -245.5306\n",
      "6900: reward: -90.00, mean_100: -85.39, episodes: 69, reward function loss: -245.5306\n",
      "7000: reward: -92.00, mean_100: -85.49, episodes: 70, reward function loss: -245.5306\n",
      "7100: reward: -88.00, mean_100: -85.52, episodes: 71, reward function loss: -245.5306\n",
      "7200: reward: -88.00, mean_100: -85.56, episodes: 72, reward function loss: -246.0837\n",
      "7300: reward: -70.00, mean_100: -85.34, episodes: 73, reward function loss: -246.0837\n",
      "7400: reward: -90.00, mean_100: -85.41, episodes: 74, reward function loss: -246.0837\n",
      "7500: reward: -90.00, mean_100: -85.47, episodes: 75, reward function loss: -246.0837\n",
      "7600: reward: -92.00, mean_100: -85.55, episodes: 76, reward function loss: -246.1159\n",
      "7700: reward: -88.00, mean_100: -85.58, episodes: 77, reward function loss: -246.1159\n",
      "7800: reward: -72.00, mean_100: -85.41, episodes: 78, reward function loss: -246.1159\n",
      "7900: reward: -86.00, mean_100: -85.42, episodes: 79, reward function loss: -246.1159\n",
      "8000: reward: -90.00, mean_100: -85.47, episodes: 80, reward function loss: -244.3880\n",
      "8100: reward: -92.00, mean_100: -85.56, episodes: 81, reward function loss: -244.3880\n",
      "8200: reward: -92.00, mean_100: -85.63, episodes: 82, reward function loss: -244.3880\n",
      "8300: reward: -58.00, mean_100: -85.30, episodes: 83, reward function loss: -244.3880\n",
      "8400: reward: -90.00, mean_100: -85.36, episodes: 84, reward function loss: -245.0418\n",
      "8500: reward: -88.00, mean_100: -85.39, episodes: 85, reward function loss: -245.0418\n",
      "8600: reward: -90.00, mean_100: -85.44, episodes: 86, reward function loss: -245.0418\n",
      "8700: reward: -92.00, mean_100: -85.52, episodes: 87, reward function loss: -245.0418\n",
      "8800: reward: -92.00, mean_100: -85.59, episodes: 88, reward function loss: -246.1231\n",
      "8900: reward: -92.00, mean_100: -85.66, episodes: 89, reward function loss: -246.1231\n",
      "9000: reward: -92.00, mean_100: -85.73, episodes: 90, reward function loss: -246.1231\n",
      "9100: reward: -92.00, mean_100: -85.80, episodes: 91, reward function loss: -246.1231\n",
      "9200: reward: -88.00, mean_100: -85.83, episodes: 92, reward function loss: -246.1225\n",
      "9300: reward: -64.00, mean_100: -85.59, episodes: 93, reward function loss: -246.1225\n",
      "9400: reward: -70.00, mean_100: -85.43, episodes: 94, reward function loss: -246.1225\n",
      "9500: reward: -68.00, mean_100: -85.24, episodes: 95, reward function loss: -246.1225\n",
      "9600: reward: -50.00, mean_100: -84.88, episodes: 96, reward function loss: -244.4649\n",
      "9700: reward: -92.00, mean_100: -84.95, episodes: 97, reward function loss: -244.4649\n",
      "9800: reward: -82.00, mean_100: -84.92, episodes: 98, reward function loss: -244.4649\n",
      "9900: reward: -62.00, mean_100: -84.69, episodes: 99, reward function loss: -244.4649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000: reward: -92.00, mean_100: -84.76, episodes: 100, reward function loss: -243.7860\n",
      "10100: reward: -64.00, mean_100: -84.48, episodes: 101, reward function loss: -243.7860\n",
      "10200: reward: -90.00, mean_100: -84.50, episodes: 102, reward function loss: -243.7860\n",
      "10300: reward: -94.00, mean_100: -84.52, episodes: 103, reward function loss: -243.7860\n",
      "10400: reward: -92.00, mean_100: -84.56, episodes: 104, reward function loss: -245.9347\n",
      "10500: reward: -64.00, mean_100: -84.52, episodes: 105, reward function loss: -245.9347\n",
      "10600: reward: -92.00, mean_100: -84.66, episodes: 106, reward function loss: -245.9347\n",
      "10700: reward: -88.00, mean_100: -84.66, episodes: 107, reward function loss: -245.9347\n",
      "10800: reward: -90.00, mean_100: -84.64, episodes: 108, reward function loss: -243.0367\n",
      "10900: reward: -74.00, mean_100: -84.48, episodes: 109, reward function loss: -243.0367\n",
      "11000: reward: -70.00, mean_100: -84.26, episodes: 110, reward function loss: -243.0367\n",
      "11100: reward: -92.00, mean_100: -84.26, episodes: 111, reward function loss: -243.0367\n",
      "11200: reward: -88.00, mean_100: -84.20, episodes: 112, reward function loss: -245.4358\n",
      "11300: reward: -84.00, mean_100: -84.20, episodes: 113, reward function loss: -245.4358\n",
      "11400: reward: -92.00, mean_100: -84.20, episodes: 114, reward function loss: -245.4358\n",
      "11500: reward: -88.00, mean_100: -84.18, episodes: 115, reward function loss: -245.4358\n",
      "11600: reward: -92.00, mean_100: -84.44, episodes: 116, reward function loss: -246.1237\n",
      "11700: reward: -66.00, mean_100: -84.20, episodes: 117, reward function loss: -246.1237\n",
      "11800: reward: -94.00, mean_100: -84.28, episodes: 118, reward function loss: -246.1237\n",
      "11900: reward: -72.00, mean_100: -84.12, episodes: 119, reward function loss: -246.1237\n",
      "12000: reward: -84.00, mean_100: -84.02, episodes: 120, reward function loss: -244.0804\n",
      "12100: reward: -86.00, mean_100: -84.02, episodes: 121, reward function loss: -244.0804\n",
      "12200: reward: -92.00, mean_100: -84.04, episodes: 122, reward function loss: -244.0804\n",
      "12300: reward: -54.00, mean_100: -83.74, episodes: 123, reward function loss: -244.0804\n",
      "12400: reward: -92.00, mean_100: -83.76, episodes: 124, reward function loss: -246.0915\n",
      "12500: reward: -78.00, mean_100: -83.62, episodes: 125, reward function loss: -246.0915\n",
      "12600: reward: -92.00, mean_100: -83.72, episodes: 126, reward function loss: -246.0915\n",
      "12700: reward: -94.00, mean_100: -83.76, episodes: 127, reward function loss: -246.0915\n",
      "12800: reward: -94.00, mean_100: -83.78, episodes: 128, reward function loss: -246.1195\n",
      "12900: reward: -90.00, mean_100: -83.76, episodes: 129, reward function loss: -246.1195\n",
      "13000: reward: -92.00, mean_100: -83.76, episodes: 130, reward function loss: -246.1195\n",
      "13100: reward: -90.00, mean_100: -83.74, episodes: 131, reward function loss: -246.1195\n",
      "13200: reward: -88.00, mean_100: -83.72, episodes: 132, reward function loss: -246.1225\n",
      "13300: reward: -92.00, mean_100: -83.96, episodes: 133, reward function loss: -246.1225\n",
      "13400: reward: -92.00, mean_100: -83.96, episodes: 134, reward function loss: -246.1225\n",
      "13500: reward: -94.00, mean_100: -84.06, episodes: 135, reward function loss: -246.1225\n",
      "13600: reward: -58.00, mean_100: -83.74, episodes: 136, reward function loss: -246.1243\n",
      "13700: reward: -92.00, mean_100: -83.78, episodes: 137, reward function loss: -246.1243\n",
      "13800: reward: -92.00, mean_100: -83.78, episodes: 138, reward function loss: -246.1243\n",
      "13900: reward: -86.00, mean_100: -83.96, episodes: 139, reward function loss: -246.1243\n",
      "14000: reward: -92.00, mean_100: -84.12, episodes: 140, reward function loss: -246.1189\n",
      "14100: reward: -92.00, mean_100: -84.24, episodes: 141, reward function loss: -246.1189\n",
      "14200: reward: -46.00, mean_100: -83.82, episodes: 142, reward function loss: -246.1189\n",
      "14300: reward: -76.00, mean_100: -83.74, episodes: 143, reward function loss: -246.1189\n",
      "14400: reward: -92.00, mean_100: -83.80, episodes: 144, reward function loss: -246.0843\n",
      "14500: reward: -52.00, mean_100: -83.42, episodes: 145, reward function loss: -246.0843\n",
      "14600: reward: -76.00, mean_100: -83.26, episodes: 146, reward function loss: -246.0843\n",
      "14700: reward: -88.00, mean_100: -83.26, episodes: 147, reward function loss: -246.0843\n",
      "14800: reward: -90.00, mean_100: -83.22, episodes: 148, reward function loss: -246.1231\n",
      "14900: reward: -92.00, mean_100: -83.64, episodes: 149, reward function loss: -246.1231\n",
      "15000: reward: -92.00, mean_100: -83.66, episodes: 150, reward function loss: -246.1231\n",
      "15100: reward: -56.00, mean_100: -83.32, episodes: 151, reward function loss: -246.1231\n",
      "15200: reward: -92.00, mean_100: -83.80, episodes: 152, reward function loss: -246.1100\n",
      "15300: reward: -88.00, mean_100: -84.04, episodes: 153, reward function loss: -246.1100\n",
      "15400: reward: -92.00, mean_100: -84.16, episodes: 154, reward function loss: -246.1100\n",
      "15500: reward: -84.00, mean_100: -84.08, episodes: 155, reward function loss: -246.1100\n",
      "15600: reward: -92.00, mean_100: -84.44, episodes: 156, reward function loss: -246.1225\n",
      "15700: reward: -84.00, mean_100: -84.42, episodes: 157, reward function loss: -246.1225\n",
      "15800: reward: -90.00, mean_100: -84.40, episodes: 158, reward function loss: -246.1225\n",
      "15900: reward: -88.00, mean_100: -84.36, episodes: 159, reward function loss: -246.1225\n",
      "16000: reward: -92.00, mean_100: -84.36, episodes: 160, reward function loss: -246.1225\n",
      "16100: reward: -92.00, mean_100: -84.36, episodes: 161, reward function loss: -246.1225\n",
      "16200: reward: -88.00, mean_100: -84.36, episodes: 162, reward function loss: -246.1225\n",
      "16300: reward: -92.00, mean_100: -84.38, episodes: 163, reward function loss: -246.1225\n",
      "16400: reward: -92.00, mean_100: -84.40, episodes: 164, reward function loss: -246.1225\n",
      "16500: reward: -92.00, mean_100: -84.52, episodes: 165, reward function loss: -246.1225\n",
      "16600: reward: -92.00, mean_100: -84.54, episodes: 166, reward function loss: -246.1225\n",
      "16700: reward: -64.00, mean_100: -84.24, episodes: 167, reward function loss: -246.1225\n",
      "16800: reward: -90.00, mean_100: -84.40, episodes: 168, reward function loss: -246.1243\n",
      "16900: reward: -74.00, mean_100: -84.24, episodes: 169, reward function loss: -246.1243\n",
      "17000: reward: -74.00, mean_100: -84.06, episodes: 170, reward function loss: -246.1243\n",
      "17100: reward: -92.00, mean_100: -84.10, episodes: 171, reward function loss: -246.1243\n",
      "17200: reward: -84.00, mean_100: -84.06, episodes: 172, reward function loss: -246.1219\n",
      "17300: reward: -90.00, mean_100: -84.26, episodes: 173, reward function loss: -246.1219\n",
      "17400: reward: -66.00, mean_100: -84.02, episodes: 174, reward function loss: -246.1219\n",
      "17500: reward: -92.00, mean_100: -84.04, episodes: 175, reward function loss: -246.1219\n",
      "17600: reward: -84.00, mean_100: -83.96, episodes: 176, reward function loss: -246.1243\n",
      "17700: reward: -76.00, mean_100: -83.84, episodes: 177, reward function loss: -246.1243\n",
      "17800: reward: -92.00, mean_100: -84.04, episodes: 178, reward function loss: -246.1243\n",
      "17900: reward: -86.00, mean_100: -84.04, episodes: 179, reward function loss: -246.1243\n",
      "18000: reward: -92.00, mean_100: -84.06, episodes: 180, reward function loss: -246.1237\n",
      "18100: reward: -92.00, mean_100: -84.06, episodes: 181, reward function loss: -246.1237\n",
      "18200: reward: -92.00, mean_100: -84.06, episodes: 182, reward function loss: -246.1237\n",
      "18300: reward: -86.00, mean_100: -84.34, episodes: 183, reward function loss: -246.1237\n",
      "18400: reward: -54.00, mean_100: -83.98, episodes: 184, reward function loss: -183.9936\n",
      "18500: reward: -92.00, mean_100: -84.02, episodes: 185, reward function loss: -183.9936\n",
      "18600: reward: -80.00, mean_100: -83.92, episodes: 186, reward function loss: -183.9936\n",
      "18700: reward: -90.00, mean_100: -83.90, episodes: 187, reward function loss: -183.9936\n",
      "18800: reward: -90.00, mean_100: -83.88, episodes: 188, reward function loss: -246.1225\n",
      "18900: reward: -86.00, mean_100: -83.82, episodes: 189, reward function loss: -246.1225\n",
      "19000: reward: -92.00, mean_100: -83.82, episodes: 190, reward function loss: -246.1225\n",
      "19100: reward: -90.00, mean_100: -83.80, episodes: 191, reward function loss: -246.1225\n",
      "19200: reward: -90.00, mean_100: -83.82, episodes: 192, reward function loss: -246.1237\n",
      "19300: reward: -92.00, mean_100: -84.10, episodes: 193, reward function loss: -246.1237\n",
      "19400: reward: -86.00, mean_100: -84.26, episodes: 194, reward function loss: -246.1237\n",
      "19500: reward: -92.00, mean_100: -84.50, episodes: 195, reward function loss: -246.1237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19600: reward: -92.00, mean_100: -84.92, episodes: 196, reward function loss: -246.1243\n",
      "19700: reward: -92.00, mean_100: -84.92, episodes: 197, reward function loss: -246.1243\n",
      "19800: reward: -92.00, mean_100: -85.02, episodes: 198, reward function loss: -246.1243\n",
      "19900: reward: -92.00, mean_100: -85.32, episodes: 199, reward function loss: -246.1243\n",
      "20000: reward: -84.00, mean_100: -85.24, episodes: 200, reward function loss: -246.1213\n",
      "20100: reward: -86.00, mean_100: -85.46, episodes: 201, reward function loss: -246.1213\n",
      "20200: reward: -92.00, mean_100: -85.48, episodes: 202, reward function loss: -246.1213\n",
      "20300: reward: -92.00, mean_100: -85.46, episodes: 203, reward function loss: -246.1213\n",
      "20400: reward: -78.00, mean_100: -85.32, episodes: 204, reward function loss: -246.1231\n",
      "20500: reward: -90.00, mean_100: -85.58, episodes: 205, reward function loss: -246.1231\n",
      "20600: reward: -88.00, mean_100: -85.54, episodes: 206, reward function loss: -246.1231\n",
      "20700: reward: -92.00, mean_100: -85.58, episodes: 207, reward function loss: -246.1231\n",
      "20800: reward: -92.00, mean_100: -85.60, episodes: 208, reward function loss: -246.1237\n",
      "20900: reward: -84.00, mean_100: -85.70, episodes: 209, reward function loss: -246.1237\n",
      "21000: reward: -90.00, mean_100: -85.90, episodes: 210, reward function loss: -246.1237\n",
      "21100: reward: -92.00, mean_100: -85.90, episodes: 211, reward function loss: -246.1237\n",
      "21200: reward: -52.00, mean_100: -85.54, episodes: 212, reward function loss: -246.1249\n",
      "21300: reward: -88.00, mean_100: -85.58, episodes: 213, reward function loss: -246.1249\n",
      "21400: reward: -88.00, mean_100: -85.54, episodes: 214, reward function loss: -246.1249\n",
      "21500: reward: -92.00, mean_100: -85.58, episodes: 215, reward function loss: -246.1249\n",
      "21600: reward: -90.00, mean_100: -85.56, episodes: 216, reward function loss: -246.1231\n",
      "21700: reward: -92.00, mean_100: -85.82, episodes: 217, reward function loss: -246.1231\n",
      "21800: reward: -84.00, mean_100: -85.72, episodes: 218, reward function loss: -246.1231\n",
      "21900: reward: -92.00, mean_100: -85.92, episodes: 219, reward function loss: -246.1231\n",
      "22000: reward: -92.00, mean_100: -86.00, episodes: 220, reward function loss: -246.1255\n",
      "22100: reward: -84.00, mean_100: -85.98, episodes: 221, reward function loss: -246.1255\n",
      "22200: reward: -88.00, mean_100: -85.94, episodes: 222, reward function loss: -246.1255\n",
      "22300: reward: -78.00, mean_100: -86.18, episodes: 223, reward function loss: -246.1255\n",
      "22400: reward: -92.00, mean_100: -86.18, episodes: 224, reward function loss: -246.1243\n",
      "22500: reward: -58.00, mean_100: -85.98, episodes: 225, reward function loss: -246.1243\n",
      "22600: reward: -92.00, mean_100: -85.98, episodes: 226, reward function loss: -246.1243\n",
      "22700: reward: -92.00, mean_100: -85.96, episodes: 227, reward function loss: -246.1243\n",
      "22800: reward: -88.00, mean_100: -85.90, episodes: 228, reward function loss: -246.1266\n",
      "22900: reward: -88.00, mean_100: -85.88, episodes: 229, reward function loss: -246.1266\n",
      "23000: reward: -92.00, mean_100: -85.88, episodes: 230, reward function loss: -246.1266\n",
      "23100: reward: -92.00, mean_100: -85.90, episodes: 231, reward function loss: -246.1266\n",
      "23200: reward: -92.00, mean_100: -85.94, episodes: 232, reward function loss: -246.1243\n",
      "23300: reward: -90.00, mean_100: -85.92, episodes: 233, reward function loss: -246.1243\n",
      "23400: reward: -92.00, mean_100: -85.92, episodes: 234, reward function loss: -246.1243\n",
      "23500: reward: -92.00, mean_100: -85.90, episodes: 235, reward function loss: -246.1243\n",
      "23600: reward: -92.00, mean_100: -86.24, episodes: 236, reward function loss: -246.1231\n",
      "23700: reward: -90.00, mean_100: -86.22, episodes: 237, reward function loss: -246.1231\n",
      "23800: reward: -90.00, mean_100: -86.20, episodes: 238, reward function loss: -246.1231\n",
      "23900: reward: -92.00, mean_100: -86.26, episodes: 239, reward function loss: -246.1231\n",
      "24000: reward: -92.00, mean_100: -86.26, episodes: 240, reward function loss: -246.1261\n",
      "24100: reward: -90.00, mean_100: -86.24, episodes: 241, reward function loss: -246.1261\n",
      "24200: reward: -92.00, mean_100: -86.70, episodes: 242, reward function loss: -246.1261\n",
      "24300: reward: -92.00, mean_100: -86.86, episodes: 243, reward function loss: -246.1261\n",
      "24400: reward: -90.00, mean_100: -86.84, episodes: 244, reward function loss: -246.1243\n",
      "24500: reward: -86.00, mean_100: -87.18, episodes: 245, reward function loss: -246.1243\n",
      "24600: reward: -92.00, mean_100: -87.34, episodes: 246, reward function loss: -246.1243\n",
      "24700: reward: -88.00, mean_100: -87.34, episodes: 247, reward function loss: -246.1243\n",
      "24800: reward: -84.00, mean_100: -87.28, episodes: 248, reward function loss: -246.1243\n",
      "24900: reward: -92.00, mean_100: -87.28, episodes: 249, reward function loss: -246.1243\n",
      "25000: reward: -92.00, mean_100: -87.28, episodes: 250, reward function loss: -246.1243\n",
      "25100: reward: -92.00, mean_100: -87.64, episodes: 251, reward function loss: -246.1243\n",
      "25200: reward: -50.00, mean_100: -87.22, episodes: 252, reward function loss: -246.1266\n",
      "25300: reward: -54.00, mean_100: -86.88, episodes: 253, reward function loss: -246.1266\n",
      "25400: reward: -88.00, mean_100: -86.84, episodes: 254, reward function loss: -246.1266\n",
      "25500: reward: -92.00, mean_100: -86.92, episodes: 255, reward function loss: -246.1266\n",
      "25600: reward: -90.00, mean_100: -86.90, episodes: 256, reward function loss: -184.0240\n",
      "25700: reward: -88.00, mean_100: -86.94, episodes: 257, reward function loss: -184.0240\n",
      "25800: reward: -56.00, mean_100: -86.60, episodes: 258, reward function loss: -184.0240\n",
      "25900: reward: -80.00, mean_100: -86.52, episodes: 259, reward function loss: -184.0240\n",
      "26000: reward: -64.00, mean_100: -86.24, episodes: 260, reward function loss: -246.1261\n",
      "26100: reward: -92.00, mean_100: -86.24, episodes: 261, reward function loss: -246.1261\n",
      "26200: reward: -92.00, mean_100: -86.28, episodes: 262, reward function loss: -246.1261\n",
      "26300: reward: -92.00, mean_100: -86.28, episodes: 263, reward function loss: -246.1261\n",
      "26400: reward: -94.00, mean_100: -86.30, episodes: 264, reward function loss: -246.1243\n",
      "26500: reward: -92.00, mean_100: -86.30, episodes: 265, reward function loss: -246.1243\n",
      "26600: reward: -88.00, mean_100: -86.26, episodes: 266, reward function loss: -246.1243\n",
      "26700: reward: -92.00, mean_100: -86.54, episodes: 267, reward function loss: -246.1243\n",
      "26800: reward: -74.00, mean_100: -86.38, episodes: 268, reward function loss: -246.1255\n",
      "26900: reward: -90.00, mean_100: -86.54, episodes: 269, reward function loss: -246.1255\n",
      "27000: reward: -90.00, mean_100: -86.70, episodes: 270, reward function loss: -246.1255\n",
      "27100: reward: -90.00, mean_100: -86.68, episodes: 271, reward function loss: -246.1255\n",
      "27200: reward: -90.00, mean_100: -86.74, episodes: 272, reward function loss: -246.1255\n",
      "27300: reward: -88.00, mean_100: -86.72, episodes: 273, reward function loss: -246.1255\n",
      "27400: reward: -50.00, mean_100: -86.56, episodes: 274, reward function loss: -246.1255\n",
      "27500: reward: -90.00, mean_100: -86.54, episodes: 275, reward function loss: -246.1255\n",
      "27600: reward: -88.00, mean_100: -86.58, episodes: 276, reward function loss: -246.1255\n",
      "27700: reward: -90.00, mean_100: -86.72, episodes: 277, reward function loss: -246.1255\n",
      "27800: reward: -88.00, mean_100: -86.68, episodes: 278, reward function loss: -246.1255\n",
      "27900: reward: -92.00, mean_100: -86.74, episodes: 279, reward function loss: -246.1255\n",
      "28000: reward: -92.00, mean_100: -86.74, episodes: 280, reward function loss: -246.1243\n",
      "28100: reward: -92.00, mean_100: -86.74, episodes: 281, reward function loss: -246.1243\n",
      "28200: reward: -92.00, mean_100: -86.74, episodes: 282, reward function loss: -246.1243\n",
      "28300: reward: -88.00, mean_100: -86.76, episodes: 283, reward function loss: -246.1243\n",
      "28400: reward: -88.00, mean_100: -87.10, episodes: 284, reward function loss: -246.1266\n",
      "28500: reward: -90.00, mean_100: -87.08, episodes: 285, reward function loss: -246.1266\n",
      "28600: reward: -86.00, mean_100: -87.14, episodes: 286, reward function loss: -246.1266\n",
      "28700: reward: -84.00, mean_100: -87.08, episodes: 287, reward function loss: -246.1266\n",
      "28800: reward: -86.00, mean_100: -87.04, episodes: 288, reward function loss: -246.1261\n",
      "28900: reward: -92.00, mean_100: -87.10, episodes: 289, reward function loss: -246.1261\n",
      "29000: reward: -48.00, mean_100: -86.66, episodes: 290, reward function loss: -246.1261\n",
      "29100: reward: -86.00, mean_100: -86.62, episodes: 291, reward function loss: -246.1261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29200: reward: -90.00, mean_100: -86.62, episodes: 292, reward function loss: -246.1255\n",
      "29300: reward: -92.00, mean_100: -86.62, episodes: 293, reward function loss: -246.1255\n",
      "29400: reward: -92.00, mean_100: -86.68, episodes: 294, reward function loss: -246.1255\n",
      "29500: reward: -92.00, mean_100: -86.68, episodes: 295, reward function loss: -246.1255\n",
      "29600: reward: -92.00, mean_100: -86.68, episodes: 296, reward function loss: -246.1272\n",
      "29700: reward: -84.00, mean_100: -86.60, episodes: 297, reward function loss: -246.1272\n",
      "29800: reward: -92.00, mean_100: -86.60, episodes: 298, reward function loss: -246.1272\n",
      "29900: reward: -66.00, mean_100: -86.34, episodes: 299, reward function loss: -246.1272\n",
      "30000: reward: -84.00, mean_100: -86.34, episodes: 300, reward function loss: -246.1261\n",
      "30100: reward: -90.00, mean_100: -86.38, episodes: 301, reward function loss: -246.1261\n",
      "30200: reward: -86.00, mean_100: -86.32, episodes: 302, reward function loss: -246.1261\n",
      "30300: reward: -90.00, mean_100: -86.30, episodes: 303, reward function loss: -246.1261\n",
      "30400: reward: -54.00, mean_100: -86.06, episodes: 304, reward function loss: -246.1272\n",
      "30500: reward: -66.00, mean_100: -85.82, episodes: 305, reward function loss: -246.1272\n",
      "30600: reward: -92.00, mean_100: -85.86, episodes: 306, reward function loss: -246.1272\n",
      "30700: reward: -94.00, mean_100: -85.88, episodes: 307, reward function loss: -246.1272\n",
      "30800: reward: -92.00, mean_100: -85.88, episodes: 308, reward function loss: -246.0861\n",
      "30900: reward: -62.00, mean_100: -85.66, episodes: 309, reward function loss: -246.0861\n",
      "31000: reward: -90.00, mean_100: -85.66, episodes: 310, reward function loss: -246.0861\n",
      "31100: reward: -92.00, mean_100: -85.66, episodes: 311, reward function loss: -246.0861\n",
      "31200: reward: -80.00, mean_100: -85.94, episodes: 312, reward function loss: -246.1225\n",
      "31300: reward: -92.00, mean_100: -85.98, episodes: 313, reward function loss: -246.1225\n",
      "31400: reward: -90.00, mean_100: -86.00, episodes: 314, reward function loss: -246.1225\n",
      "31500: reward: -90.00, mean_100: -85.98, episodes: 315, reward function loss: -246.1225\n",
      "31600: reward: -92.00, mean_100: -86.00, episodes: 316, reward function loss: -246.1237\n",
      "31700: reward: -76.00, mean_100: -85.84, episodes: 317, reward function loss: -246.1237\n",
      "31800: reward: -92.00, mean_100: -85.92, episodes: 318, reward function loss: -246.1237\n",
      "31900: reward: -52.00, mean_100: -85.52, episodes: 319, reward function loss: -246.1237\n",
      "32000: reward: -92.00, mean_100: -85.52, episodes: 320, reward function loss: -246.1272\n",
      "32100: reward: -90.00, mean_100: -85.58, episodes: 321, reward function loss: -246.1272\n",
      "32200: reward: -90.00, mean_100: -85.60, episodes: 322, reward function loss: -246.1272\n",
      "32300: reward: -90.00, mean_100: -85.72, episodes: 323, reward function loss: -246.1272\n",
      "32400: reward: -92.00, mean_100: -85.72, episodes: 324, reward function loss: -246.1231\n",
      "32500: reward: -88.00, mean_100: -86.02, episodes: 325, reward function loss: -246.1231\n",
      "32600: reward: -92.00, mean_100: -86.02, episodes: 326, reward function loss: -246.1231\n",
      "32700: reward: -92.00, mean_100: -86.02, episodes: 327, reward function loss: -246.1231\n",
      "32800: reward: -88.00, mean_100: -86.02, episodes: 328, reward function loss: -246.1237\n",
      "32900: reward: -90.00, mean_100: -86.04, episodes: 329, reward function loss: -246.1237\n",
      "33000: reward: -90.00, mean_100: -86.02, episodes: 330, reward function loss: -246.1237\n",
      "33100: reward: -92.00, mean_100: -86.02, episodes: 331, reward function loss: -246.1237\n",
      "33200: reward: -92.00, mean_100: -86.02, episodes: 332, reward function loss: -246.1255\n",
      "33300: reward: -92.00, mean_100: -86.04, episodes: 333, reward function loss: -246.1255\n",
      "33400: reward: -92.00, mean_100: -86.04, episodes: 334, reward function loss: -246.1255\n",
      "33500: reward: -92.00, mean_100: -86.04, episodes: 335, reward function loss: -246.1255\n",
      "33600: reward: -92.00, mean_100: -86.04, episodes: 336, reward function loss: -246.1249\n",
      "33700: reward: -92.00, mean_100: -86.06, episodes: 337, reward function loss: -246.1249\n",
      "33800: reward: -92.00, mean_100: -86.08, episodes: 338, reward function loss: -246.1249\n",
      "33900: reward: -86.00, mean_100: -86.02, episodes: 339, reward function loss: -246.1249\n",
      "34000: reward: -90.00, mean_100: -86.00, episodes: 340, reward function loss: -246.1243\n",
      "34100: reward: -90.00, mean_100: -86.00, episodes: 341, reward function loss: -246.1243\n",
      "34200: reward: -92.00, mean_100: -86.00, episodes: 342, reward function loss: -246.1243\n",
      "34300: reward: -92.00, mean_100: -86.00, episodes: 343, reward function loss: -246.1243\n",
      "34400: reward: -92.00, mean_100: -86.02, episodes: 344, reward function loss: -246.1255\n",
      "34500: reward: -92.00, mean_100: -86.08, episodes: 345, reward function loss: -246.1255\n",
      "34600: reward: -88.00, mean_100: -86.04, episodes: 346, reward function loss: -246.1255\n",
      "34700: reward: -92.00, mean_100: -86.08, episodes: 347, reward function loss: -246.1255\n",
      "34800: reward: -92.00, mean_100: -86.16, episodes: 348, reward function loss: -246.1249\n",
      "34900: reward: -92.00, mean_100: -86.16, episodes: 349, reward function loss: -246.1249\n",
      "35000: reward: -92.00, mean_100: -86.16, episodes: 350, reward function loss: -246.1249\n",
      "35100: reward: -90.00, mean_100: -86.14, episodes: 351, reward function loss: -246.1249\n",
      "35200: reward: -92.00, mean_100: -86.56, episodes: 352, reward function loss: -246.1255\n",
      "35300: reward: -92.00, mean_100: -86.94, episodes: 353, reward function loss: -246.1255\n",
      "35400: reward: -94.00, mean_100: -87.00, episodes: 354, reward function loss: -246.1255\n",
      "35500: reward: -92.00, mean_100: -87.00, episodes: 355, reward function loss: -246.1255\n",
      "35600: reward: -78.00, mean_100: -86.88, episodes: 356, reward function loss: -246.1261\n",
      "35700: reward: -84.00, mean_100: -86.84, episodes: 357, reward function loss: -246.1261\n",
      "35800: reward: -92.00, mean_100: -87.20, episodes: 358, reward function loss: -246.1261\n",
      "35900: reward: -92.00, mean_100: -87.32, episodes: 359, reward function loss: -246.1261\n",
      "36000: reward: -88.00, mean_100: -87.56, episodes: 360, reward function loss: -246.1261\n",
      "36100: reward: -90.00, mean_100: -87.54, episodes: 361, reward function loss: -246.1261\n",
      "36200: reward: -92.00, mean_100: -87.54, episodes: 362, reward function loss: -246.1261\n",
      "36300: reward: -92.00, mean_100: -87.54, episodes: 363, reward function loss: -246.1261\n",
      "36400: reward: -90.00, mean_100: -87.50, episodes: 364, reward function loss: -246.1255\n",
      "36500: reward: -92.00, mean_100: -87.50, episodes: 365, reward function loss: -246.1255\n",
      "36600: reward: -92.00, mean_100: -87.54, episodes: 366, reward function loss: -246.1255\n",
      "36700: reward: -92.00, mean_100: -87.54, episodes: 367, reward function loss: -246.1255\n",
      "36800: reward: -90.00, mean_100: -87.70, episodes: 368, reward function loss: -246.1255\n",
      "36900: reward: -92.00, mean_100: -87.72, episodes: 369, reward function loss: -246.1255\n",
      "37000: reward: -88.00, mean_100: -87.70, episodes: 370, reward function loss: -246.1255\n",
      "37100: reward: -84.00, mean_100: -87.64, episodes: 371, reward function loss: -246.1255\n",
      "37200: reward: -92.00, mean_100: -87.66, episodes: 372, reward function loss: -246.1249\n",
      "37300: reward: -48.00, mean_100: -87.26, episodes: 373, reward function loss: -246.1249\n",
      "37400: reward: -90.00, mean_100: -87.66, episodes: 374, reward function loss: -246.1249\n",
      "37500: reward: -90.00, mean_100: -87.66, episodes: 375, reward function loss: -246.1249\n",
      "37600: reward: -90.00, mean_100: -87.68, episodes: 376, reward function loss: -246.1261\n",
      "37700: reward: -92.00, mean_100: -87.70, episodes: 377, reward function loss: -246.1261\n",
      "37800: reward: -92.00, mean_100: -87.74, episodes: 378, reward function loss: -246.1261\n",
      "37900: reward: -68.00, mean_100: -87.50, episodes: 379, reward function loss: -246.1261\n",
      "38000: reward: -92.00, mean_100: -87.50, episodes: 380, reward function loss: -246.1266\n",
      "38100: reward: -92.00, mean_100: -87.50, episodes: 381, reward function loss: -246.1266\n",
      "38200: reward: -92.00, mean_100: -87.50, episodes: 382, reward function loss: -246.1266\n",
      "38300: reward: -92.00, mean_100: -87.54, episodes: 383, reward function loss: -246.1266\n",
      "38400: reward: -90.00, mean_100: -87.56, episodes: 384, reward function loss: -246.1261\n",
      "38500: reward: -88.00, mean_100: -87.54, episodes: 385, reward function loss: -246.1261\n",
      "38600: reward: -90.00, mean_100: -87.58, episodes: 386, reward function loss: -246.1261\n",
      "38700: reward: -94.00, mean_100: -87.68, episodes: 387, reward function loss: -246.1261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38800: reward: -92.00, mean_100: -87.74, episodes: 388, reward function loss: -246.1266\n",
      "38900: reward: -92.00, mean_100: -87.74, episodes: 389, reward function loss: -246.1266\n",
      "39000: reward: -88.00, mean_100: -88.14, episodes: 390, reward function loss: -246.1266\n",
      "39100: reward: -92.00, mean_100: -88.20, episodes: 391, reward function loss: -246.1266\n",
      "39200: reward: -80.00, mean_100: -88.10, episodes: 392, reward function loss: -246.1272\n",
      "39300: reward: -84.00, mean_100: -88.02, episodes: 393, reward function loss: -246.1272\n",
      "39400: reward: -92.00, mean_100: -88.02, episodes: 394, reward function loss: -246.1272\n",
      "39500: reward: -92.00, mean_100: -88.02, episodes: 395, reward function loss: -246.1272\n",
      "39600: reward: -92.00, mean_100: -88.02, episodes: 396, reward function loss: -246.1261\n",
      "39700: reward: -92.00, mean_100: -88.10, episodes: 397, reward function loss: -246.1261\n",
      "39800: reward: -86.00, mean_100: -88.04, episodes: 398, reward function loss: -246.1261\n",
      "39900: reward: -88.00, mean_100: -88.26, episodes: 399, reward function loss: -246.1261\n",
      "40000: reward: -84.00, mean_100: -88.26, episodes: 400, reward function loss: -246.1261\n",
      "40100: reward: -92.00, mean_100: -88.28, episodes: 401, reward function loss: -246.1261\n",
      "40200: reward: -50.00, mean_100: -87.92, episodes: 402, reward function loss: -246.1261\n",
      "40300: reward: -86.00, mean_100: -87.88, episodes: 403, reward function loss: -246.1261\n",
      "40400: reward: -92.00, mean_100: -88.26, episodes: 404, reward function loss: -246.1255\n",
      "40500: reward: -92.00, mean_100: -88.52, episodes: 405, reward function loss: -246.1255\n",
      "40600: reward: -92.00, mean_100: -88.52, episodes: 406, reward function loss: -246.1255\n",
      "40700: reward: -88.00, mean_100: -88.46, episodes: 407, reward function loss: -246.1255\n",
      "40800: reward: -92.00, mean_100: -88.46, episodes: 408, reward function loss: -246.1243\n",
      "40900: reward: -80.00, mean_100: -88.64, episodes: 409, reward function loss: -246.1243\n",
      "41000: reward: -92.00, mean_100: -88.66, episodes: 410, reward function loss: -246.1243\n",
      "41100: reward: -94.00, mean_100: -88.68, episodes: 411, reward function loss: -246.1243\n",
      "41200: reward: -90.00, mean_100: -88.78, episodes: 412, reward function loss: -246.1266\n",
      "41300: reward: -84.00, mean_100: -88.70, episodes: 413, reward function loss: -246.1266\n",
      "41400: reward: -92.00, mean_100: -88.72, episodes: 414, reward function loss: -246.1266\n",
      "41500: reward: -92.00, mean_100: -88.74, episodes: 415, reward function loss: -246.1266\n",
      "41600: reward: -84.00, mean_100: -88.66, episodes: 416, reward function loss: -246.1255\n",
      "41700: reward: -92.00, mean_100: -88.82, episodes: 417, reward function loss: -246.1255\n",
      "41800: reward: -90.00, mean_100: -88.80, episodes: 418, reward function loss: -246.1255\n",
      "41900: reward: -92.00, mean_100: -89.20, episodes: 419, reward function loss: -246.1255\n",
      "42000: reward: -86.00, mean_100: -89.14, episodes: 420, reward function loss: -246.1255\n",
      "42100: reward: -92.00, mean_100: -89.16, episodes: 421, reward function loss: -246.1255\n",
      "42200: reward: -86.00, mean_100: -89.12, episodes: 422, reward function loss: -246.1255\n",
      "42300: reward: -92.00, mean_100: -89.14, episodes: 423, reward function loss: -246.1255\n",
      "42400: reward: -88.00, mean_100: -89.10, episodes: 424, reward function loss: -246.1255\n",
      "42500: reward: -90.00, mean_100: -89.12, episodes: 425, reward function loss: -246.1255\n",
      "42600: reward: -92.00, mean_100: -89.12, episodes: 426, reward function loss: -246.1255\n",
      "42700: reward: -92.00, mean_100: -89.12, episodes: 427, reward function loss: -246.1255\n",
      "42800: reward: -90.00, mean_100: -89.14, episodes: 428, reward function loss: -246.1266\n",
      "42900: reward: -66.00, mean_100: -88.90, episodes: 429, reward function loss: -246.1266\n",
      "43000: reward: -92.00, mean_100: -88.92, episodes: 430, reward function loss: -246.1266\n",
      "43100: reward: -92.00, mean_100: -88.92, episodes: 431, reward function loss: -246.1266\n",
      "43200: reward: -90.00, mean_100: -88.90, episodes: 432, reward function loss: -246.1261\n",
      "43300: reward: -92.00, mean_100: -88.90, episodes: 433, reward function loss: -246.1261\n",
      "43400: reward: -90.00, mean_100: -88.88, episodes: 434, reward function loss: -246.1261\n",
      "43500: reward: -76.00, mean_100: -88.72, episodes: 435, reward function loss: -246.1261\n",
      "43600: reward: -86.00, mean_100: -88.66, episodes: 436, reward function loss: -246.1261\n",
      "43700: reward: -90.00, mean_100: -88.64, episodes: 437, reward function loss: -246.1261\n",
      "43800: reward: -92.00, mean_100: -88.64, episodes: 438, reward function loss: -246.1261\n",
      "43900: reward: -94.00, mean_100: -88.72, episodes: 439, reward function loss: -246.1261\n",
      "44000: reward: -88.00, mean_100: -88.70, episodes: 440, reward function loss: -246.1266\n",
      "44100: reward: -90.00, mean_100: -88.70, episodes: 441, reward function loss: -246.1266\n",
      "44200: reward: -92.00, mean_100: -88.70, episodes: 442, reward function loss: -246.1266\n",
      "44300: reward: -88.00, mean_100: -88.66, episodes: 443, reward function loss: -246.1266\n",
      "44400: reward: -92.00, mean_100: -88.66, episodes: 444, reward function loss: -184.0234\n",
      "44500: reward: -88.00, mean_100: -88.62, episodes: 445, reward function loss: -184.0234\n",
      "44600: reward: -92.00, mean_100: -88.66, episodes: 446, reward function loss: -184.0234\n",
      "44700: reward: -92.00, mean_100: -88.66, episodes: 447, reward function loss: -184.0234\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-8dee45f7a7e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mdemo_batch_actions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdemo_actions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mdemo_batch_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdemo_batch_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mdemo_batch_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdemo_batch_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_rewards = []\n",
    "step_idx = 0\n",
    "done_episodes = 0\n",
    "\n",
    "batch_episodes = 0\n",
    "batch_states, batch_actions, batch_qvals = [], [], []\n",
    "cur_rewards = []\n",
    "loss_rwd = 0.\n",
    "\n",
    "for step_idx, exp in enumerate(exp_source):\n",
    "    batch_states.append(exp.state)\n",
    "    batch_actions.append(int(exp.action))\n",
    "    x = torch.cat([float32_preprocessor(exp.state), float32_preprocessor([int(exp.action)])]).view(1, -1)\n",
    "    reward = reward_net(x)\n",
    "    cur_rewards.append(reward.item())\n",
    "\n",
    "    if exp.last_state is None:\n",
    "        batch_qvals.extend(calc_qvals(cur_rewards))\n",
    "        cur_rewards.clear()\n",
    "        batch_episodes += 1\n",
    "\n",
    "    new_rewards = exp_source.pop_total_rewards()\n",
    "    if new_rewards:\n",
    "        done_episodes += 1\n",
    "        reward = new_rewards[0]\n",
    "        total_rewards.append(reward)\n",
    "        mean_rewards = float(np.mean(total_rewards[-100:]))\n",
    "        writer.add_scalar('reward', reward, done_episodes)\n",
    "        writer.add_scalar('mean_reward', mean_rewards, done_episodes)\n",
    "        writer.add_scalar('loss_rwd', loss_rwd, done_episodes)        \n",
    "        print(f'{step_idx}: reward: {reward:6.2f}, mean_100: {mean_rewards:6.2f}, '\n",
    "              f'episodes: {done_episodes}, reward function loss: {loss_rwd:6.4f}')\n",
    "        \n",
    "        ## Tensorboard logging \n",
    "        if done_episodes%100 == 0 or mean_rewards>=80:\n",
    "            S1 = np.linspace(0, 1, 100)\n",
    "            S2 = np.linspace(-1, 1, 100)\n",
    "            S3 = 0*S1\n",
    "            S4 = 0*S1\n",
    "            A1 = np.ones(100)\n",
    "            Reward = np.zeros((100,100))\n",
    "            for i in range(100):\n",
    "                for j in range(100):\n",
    "                    state = [S1[i], S2[j], 0, 0]\n",
    "                    action = 1\n",
    "                    x = torch.cat([float32_preprocessor(state), float32_preprocessor([int(action)])]).view(1, -1)\n",
    "                    r = reward_net(x)\n",
    "                    Reward[i,j] = r\n",
    "                    \n",
    "            X, Y = np.meshgrid(S1, S2)\n",
    "            Z = Reward\n",
    "            fig = plt.figure()\n",
    "            ax = plt.axes(projection='3d')\n",
    "            ax.plot_surface(X, Y, Z, rstride=1, cstride=1,\n",
    "                            cmap='viridis', edgecolor='none')\n",
    "            ax.set_title('surface');\n",
    "            ax.set_xlabel('x')\n",
    "            ax.set_ylabel('y')\n",
    "            ax.set_zlabel('z');\n",
    "            ax.view_init(azim=0, elev=90)\n",
    "            writer.add_figure('mesh', fig, global_step=done_episodes/100)\n",
    "        \n",
    "        if mean_rewards >= 80:\n",
    "            print(f'Solved in {step_idx} steps and {done_episodes} episodes!')\n",
    "            torch.save(agent_net.state_dict(), 'cartpole_learner.mod')\n",
    "            torch.save(reward_net.state_dict(), 'cartpole-v1_reward_func.mod')\n",
    "            break\n",
    "\n",
    "    if batch_episodes < EPISODES_TO_TRAIN:\n",
    "        continue\n",
    "\n",
    "    states_v = torch.FloatTensor(batch_states)\n",
    "    batch_actions_t = torch.LongTensor(batch_actions)\n",
    "    batch_qvals_v = torch.FloatTensor(batch_qvals)\n",
    "  \n",
    "    for rf_i in range(10):\n",
    "        # reward function learning\n",
    "        demo_states = np.array(demonstrations['states'])\n",
    "        demo_actions = np.array(demonstrations['actions'])\n",
    "        selected = np.random.choice(len(demonstrations['states']), DEMO_BATCH)\n",
    "        demo_states = demo_states[selected]\n",
    "        demo_actions = demo_actions[selected]\n",
    "        demo_batch_states, demo_batch_actions = [], []\n",
    "    \n",
    "        for idx in range(len(demo_states)):\n",
    "            demo_batch_states.extend(demo_states[idx])\n",
    "            demo_batch_actions.extend(demo_actions[idx])\n",
    "        \n",
    "        demo_batch_states = torch.FloatTensor(demo_batch_states)\n",
    "        demo_batch_actions = torch.FloatTensor(demo_batch_actions)\n",
    "        \n",
    "        D_demo = torch.cat([demo_batch_states, demo_batch_actions.view(-1, 1)], dim=-1)\n",
    "        D_samp = torch.cat([states_v, batch_actions_t.float().view(-1, 1)], dim=-1)\n",
    "        D_samp = torch.cat([D_demo, D_samp])\n",
    "        \n",
    "        # dummy importance weights - fix later\n",
    "        z = torch.ones((D_samp.shape[0], 1))\n",
    "\n",
    "        # objective\n",
    "        D_demo_out = reward_net(D_demo)\n",
    "        D_samp_out = reward_net(D_samp)\n",
    "        D_samp_out = z * torch.exp(D_samp_out)\n",
    "        loss_rwd = torch.mean(D_demo_out) - torch.log(torch.mean(D_samp_out))\n",
    "        loss_rwd = -10000*loss_rwd  # for maximization\n",
    "\n",
    "        # update parameters\n",
    "        optimizer_reward.zero_grad()\n",
    "        loss_rwd.backward()\n",
    "        optimizer_reward.step()\n",
    "\n",
    "    # agent\n",
    "    optimizer_agent.zero_grad()\n",
    "    logits_v = agent_net(states_v)\n",
    "    log_prob_v = torch.log_softmax(logits_v, dim=1)\n",
    "    # REINFORCE\n",
    "    log_prob_actions_v = batch_qvals_v * log_prob_v[range(len(batch_states)), batch_actions_t]\n",
    "    loss_v = log_prob_actions_v.mean()\n",
    "\n",
    "    loss_v.backward()\n",
    "    optimizer_agent.step()\n",
    "\n",
    "    batch_episodes = 0\n",
    "    batch_states.clear()\n",
    "    batch_actions.clear()\n",
    "    batch_qvals.clear()\n",
    "env.close()\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
