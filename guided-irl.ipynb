{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "from collections import namedtuple\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import ptan\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from ptan.agent import float32_preprocessor\n",
    "\n",
    "from util import PGN, RewardNet\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.01\n",
    "EPISODES_TO_TRAIN = 4\n",
    "DEMO_BATCH = 50\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['state', 'action', 'reward', 'next_state'])\n",
    "Trajectory = namedtuple('Trajectory', field_names=['prob', 'episode_steps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_qvals(rewards):\n",
    "    res = []\n",
    "    sum_r = 0.0\n",
    "    for r in reversed(rewards):\n",
    "        sum_r *= GAMMA\n",
    "        sum_r += r\n",
    "        res.append(sum_r)\n",
    "    return list(reversed(res))\n",
    "\n",
    "\n",
    "def process_demonstrations(demo_samples):\n",
    "    traj_states, traj_actions, traj_qvals, traj_prob = [], [], [], []\n",
    "    for traj in demo_samples:\n",
    "        states, actions, rewards, qvals = [], [], [], []\n",
    "        traj_prob.append(traj.prob)\n",
    "        for step in traj.episode_steps:\n",
    "            states.append(step.state)\n",
    "            actions.append(step.action)\n",
    "            rewards.append(step.reward)\n",
    "        qvals.extend(calc_qvals(rewards))\n",
    "\n",
    "        traj_states.append(states)\n",
    "        traj_actions.append(actions)\n",
    "        traj_qvals.append(qvals)\n",
    "    traj_states = np.array(traj_states, dtype=np.object)\n",
    "    traj_actions = np.array(traj_actions, dtype=np.object)\n",
    "    traj_qvals = np.array(traj_qvals, dtype=np.object)\n",
    "    traj_prob = np.array(traj_prob, dtype=np.float)\n",
    "    return {'states': traj_states, 'actions': traj_actions, 'qvals': traj_qvals, 'traj_probs': traj_prob}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "agent_net = PGN(env.observation_space.shape[0], env.action_space.n)\n",
    "reward_net = RewardNet(env.observation_space.shape[0] + 1)\n",
    "agent = ptan.agent.PolicyAgent(agent_net, preprocessor=float32_preprocessor, apply_softmax=True)\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=GAMMA)\n",
    "optimizer_agent = optim.Adam(agent_net.parameters(), lr=LEARNING_RATE)\n",
    "optimizer_reward = optim.Adam(reward_net.parameters(), lr=1e-2, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of demonstrations: 100\n"
     ]
    }
   ],
   "source": [
    "with open('demonstrations.list.pkl', 'rb') as f:\n",
    "    demonstrations = pickle.load(f)\n",
    "assert (len(demonstrations) > DEMO_BATCH)\n",
    "print(f'Number of demonstrations: {len(demonstrations)}')\n",
    "demonstrations = process_demonstrations(demonstrations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12: reward:  12.00, mean_100:  12.00, episodes: 1, reward function loss: 0.0000\n",
      "29: reward:  17.00, mean_100:  14.50, episodes: 2, reward function loss: 0.0000\n",
      "60: reward:  31.00, mean_100:  20.00, episodes: 3, reward function loss: 0.0000\n",
      "73: reward:  13.00, mean_100:  18.25, episodes: 4, reward function loss: 0.0001\n",
      "104: reward:  31.00, mean_100:  20.80, episodes: 5, reward function loss: 0.0001\n",
      "126: reward:  22.00, mean_100:  21.00, episodes: 6, reward function loss: 0.0001\n",
      "137: reward:  11.00, mean_100:  19.57, episodes: 7, reward function loss: 0.0001\n",
      "151: reward:  14.00, mean_100:  18.88, episodes: 8, reward function loss: -0.0002\n",
      "172: reward:  21.00, mean_100:  19.11, episodes: 9, reward function loss: -0.0002\n",
      "210: reward:  38.00, mean_100:  21.00, episodes: 10, reward function loss: -0.0002\n",
      "269: reward:  59.00, mean_100:  24.45, episodes: 11, reward function loss: -0.0002\n",
      "300: reward:  31.00, mean_100:  25.00, episodes: 12, reward function loss: -0.0003\n",
      "316: reward:  16.00, mean_100:  24.31, episodes: 13, reward function loss: -0.0003\n",
      "349: reward:  33.00, mean_100:  24.93, episodes: 14, reward function loss: -0.0003\n",
      "387: reward:  38.00, mean_100:  25.80, episodes: 15, reward function loss: -0.0003\n",
      "405: reward:  18.00, mean_100:  25.31, episodes: 16, reward function loss: -0.0003\n",
      "424: reward:  19.00, mean_100:  24.94, episodes: 17, reward function loss: -0.0003\n",
      "447: reward:  23.00, mean_100:  24.83, episodes: 18, reward function loss: -0.0003\n",
      "463: reward:  16.00, mean_100:  24.37, episodes: 19, reward function loss: -0.0003\n",
      "510: reward:  47.00, mean_100:  25.50, episodes: 20, reward function loss: -0.0007\n",
      "543: reward:  33.00, mean_100:  25.86, episodes: 21, reward function loss: -0.0007\n",
      "575: reward:  32.00, mean_100:  26.14, episodes: 22, reward function loss: -0.0007\n",
      "620: reward:  45.00, mean_100:  26.96, episodes: 23, reward function loss: -0.0007\n",
      "637: reward:  17.00, mean_100:  26.54, episodes: 24, reward function loss: -0.0009\n",
      "704: reward:  67.00, mean_100:  28.16, episodes: 25, reward function loss: -0.0009\n",
      "733: reward:  29.00, mean_100:  28.19, episodes: 26, reward function loss: -0.0009\n",
      "756: reward:  23.00, mean_100:  28.00, episodes: 27, reward function loss: -0.0009\n",
      "781: reward:  25.00, mean_100:  27.89, episodes: 28, reward function loss: -0.0006\n",
      "803: reward:  22.00, mean_100:  27.69, episodes: 29, reward function loss: -0.0006\n",
      "846: reward:  43.00, mean_100:  28.20, episodes: 30, reward function loss: -0.0006\n",
      "878: reward:  32.00, mean_100:  28.32, episodes: 31, reward function loss: -0.0006\n",
      "895: reward:  17.00, mean_100:  27.97, episodes: 32, reward function loss: -0.0013\n",
      "968: reward:  73.00, mean_100:  29.33, episodes: 33, reward function loss: -0.0013\n",
      "1016: reward:  48.00, mean_100:  29.88, episodes: 34, reward function loss: -0.0013\n",
      "1090: reward:  74.00, mean_100:  31.14, episodes: 35, reward function loss: -0.0013\n",
      "1122: reward:  32.00, mean_100:  31.17, episodes: 36, reward function loss: -0.0025\n",
      "1204: reward:  82.00, mean_100:  32.54, episodes: 37, reward function loss: -0.0025\n",
      "1264: reward:  60.00, mean_100:  33.26, episodes: 38, reward function loss: -0.0025\n",
      "1289: reward:  25.00, mean_100:  33.05, episodes: 39, reward function loss: -0.0025\n",
      "1336: reward:  47.00, mean_100:  33.40, episodes: 40, reward function loss: -0.0015\n",
      "1401: reward:  65.00, mean_100:  34.17, episodes: 41, reward function loss: -0.0015\n",
      "1441: reward:  40.00, mean_100:  34.31, episodes: 42, reward function loss: -0.0015\n",
      "1491: reward:  50.00, mean_100:  34.67, episodes: 43, reward function loss: -0.0015\n",
      "1552: reward:  61.00, mean_100:  35.27, episodes: 44, reward function loss: -0.0028\n",
      "1581: reward:  29.00, mean_100:  35.13, episodes: 45, reward function loss: -0.0028\n",
      "1646: reward:  65.00, mean_100:  35.78, episodes: 46, reward function loss: -0.0028\n",
      "1682: reward:  36.00, mean_100:  35.79, episodes: 47, reward function loss: -0.0028\n",
      "1774: reward:  92.00, mean_100:  36.96, episodes: 48, reward function loss: -0.0020\n",
      "1791: reward:  17.00, mean_100:  36.55, episodes: 49, reward function loss: -0.0020\n",
      "1929: reward: 138.00, mean_100:  38.58, episodes: 50, reward function loss: -0.0020\n",
      "1971: reward:  42.00, mean_100:  38.65, episodes: 51, reward function loss: -0.0020\n",
      "2028: reward:  57.00, mean_100:  39.00, episodes: 52, reward function loss: -0.0016\n",
      "2128: reward: 100.00, mean_100:  40.15, episodes: 53, reward function loss: -0.0016\n",
      "2170: reward:  42.00, mean_100:  40.19, episodes: 54, reward function loss: -0.0016\n",
      "2219: reward:  49.00, mean_100:  40.35, episodes: 55, reward function loss: -0.0016\n",
      "2299: reward:  80.00, mean_100:  41.05, episodes: 56, reward function loss: -0.0025\n",
      "2345: reward:  46.00, mean_100:  41.14, episodes: 57, reward function loss: -0.0025\n",
      "2436: reward:  91.00, mean_100:  42.00, episodes: 58, reward function loss: -0.0025\n",
      "2551: reward: 115.00, mean_100:  43.24, episodes: 59, reward function loss: -0.0025\n",
      "2630: reward:  79.00, mean_100:  43.83, episodes: 60, reward function loss: -0.0075\n",
      "2740: reward: 110.00, mean_100:  44.92, episodes: 61, reward function loss: -0.0075\n",
      "2792: reward:  52.00, mean_100:  45.03, episodes: 62, reward function loss: -0.0075\n",
      "2832: reward:  40.00, mean_100:  44.95, episodes: 63, reward function loss: -0.0075\n",
      "2943: reward: 111.00, mean_100:  45.98, episodes: 64, reward function loss: -0.0044\n",
      "3025: reward:  82.00, mean_100:  46.54, episodes: 65, reward function loss: -0.0044\n",
      "3077: reward:  52.00, mean_100:  46.62, episodes: 66, reward function loss: -0.0044\n",
      "3122: reward:  45.00, mean_100:  46.60, episodes: 67, reward function loss: -0.0044\n",
      "3204: reward:  82.00, mean_100:  47.12, episodes: 68, reward function loss: -0.0045\n",
      "3254: reward:  50.00, mean_100:  47.16, episodes: 69, reward function loss: -0.0045\n",
      "3296: reward:  42.00, mean_100:  47.09, episodes: 70, reward function loss: -0.0045\n",
      "3328: reward:  32.00, mean_100:  46.87, episodes: 71, reward function loss: -0.0045\n",
      "3353: reward:  25.00, mean_100:  46.57, episodes: 72, reward function loss: -0.0013\n",
      "3387: reward:  34.00, mean_100:  46.40, episodes: 73, reward function loss: -0.0013\n",
      "3441: reward:  54.00, mean_100:  46.50, episodes: 74, reward function loss: -0.0013\n",
      "3531: reward:  90.00, mean_100:  47.08, episodes: 75, reward function loss: -0.0013\n",
      "3596: reward:  65.00, mean_100:  47.32, episodes: 76, reward function loss: -0.0016\n",
      "3622: reward:  26.00, mean_100:  47.04, episodes: 77, reward function loss: -0.0016\n",
      "3649: reward:  27.00, mean_100:  46.78, episodes: 78, reward function loss: -0.0016\n",
      "3748: reward:  99.00, mean_100:  47.44, episodes: 79, reward function loss: -0.0016\n",
      "3834: reward:  86.00, mean_100:  47.92, episodes: 80, reward function loss: -0.0021\n",
      "3878: reward:  44.00, mean_100:  47.88, episodes: 81, reward function loss: -0.0021\n",
      "3938: reward:  60.00, mean_100:  48.02, episodes: 82, reward function loss: -0.0021\n",
      "3969: reward:  31.00, mean_100:  47.82, episodes: 83, reward function loss: -0.0021\n",
      "4024: reward:  55.00, mean_100:  47.90, episodes: 84, reward function loss: -0.0017\n",
      "4081: reward:  57.00, mean_100:  48.01, episodes: 85, reward function loss: -0.0017\n",
      "4130: reward:  49.00, mean_100:  48.02, episodes: 86, reward function loss: -0.0017\n",
      "4162: reward:  32.00, mean_100:  47.84, episodes: 87, reward function loss: -0.0017\n",
      "4209: reward:  47.00, mean_100:  47.83, episodes: 88, reward function loss: -0.0029\n",
      "4274: reward:  65.00, mean_100:  48.02, episodes: 89, reward function loss: -0.0029\n",
      "4339: reward:  65.00, mean_100:  48.21, episodes: 90, reward function loss: -0.0029\n",
      "4389: reward:  50.00, mean_100:  48.23, episodes: 91, reward function loss: -0.0029\n",
      "4416: reward:  27.00, mean_100:  48.00, episodes: 92, reward function loss: -0.0020\n",
      "4444: reward:  28.00, mean_100:  47.78, episodes: 93, reward function loss: -0.0020\n",
      "4494: reward:  50.00, mean_100:  47.81, episodes: 94, reward function loss: -0.0020\n",
      "4542: reward:  48.00, mean_100:  47.81, episodes: 95, reward function loss: -0.0020\n",
      "4583: reward:  41.00, mean_100:  47.74, episodes: 96, reward function loss: -0.0015\n",
      "4619: reward:  36.00, mean_100:  47.62, episodes: 97, reward function loss: -0.0015\n",
      "4647: reward:  28.00, mean_100:  47.42, episodes: 98, reward function loss: -0.0015\n",
      "4693: reward:  46.00, mean_100:  47.40, episodes: 99, reward function loss: -0.0015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4734: reward:  41.00, mean_100:  47.34, episodes: 100, reward function loss: -0.0014\n",
      "4762: reward:  28.00, mean_100:  47.50, episodes: 101, reward function loss: -0.0014\n",
      "4796: reward:  34.00, mean_100:  47.67, episodes: 102, reward function loss: -0.0014\n",
      "4858: reward:  62.00, mean_100:  47.98, episodes: 103, reward function loss: -0.0014\n",
      "4926: reward:  68.00, mean_100:  48.53, episodes: 104, reward function loss: -0.0029\n",
      "4953: reward:  27.00, mean_100:  48.49, episodes: 105, reward function loss: -0.0029\n",
      "4982: reward:  29.00, mean_100:  48.56, episodes: 106, reward function loss: -0.0029\n",
      "5013: reward:  31.00, mean_100:  48.76, episodes: 107, reward function loss: -0.0029\n",
      "5083: reward:  70.00, mean_100:  49.32, episodes: 108, reward function loss: -0.0029\n",
      "5145: reward:  62.00, mean_100:  49.73, episodes: 109, reward function loss: -0.0029\n",
      "5167: reward:  22.00, mean_100:  49.57, episodes: 110, reward function loss: -0.0029\n",
      "5206: reward:  39.00, mean_100:  49.37, episodes: 111, reward function loss: -0.0029\n",
      "5242: reward:  36.00, mean_100:  49.42, episodes: 112, reward function loss: -0.0011\n",
      "5283: reward:  41.00, mean_100:  49.67, episodes: 113, reward function loss: -0.0011\n",
      "5333: reward:  50.00, mean_100:  49.84, episodes: 114, reward function loss: -0.0011\n",
      "5379: reward:  46.00, mean_100:  49.92, episodes: 115, reward function loss: -0.0011\n",
      "5415: reward:  36.00, mean_100:  50.10, episodes: 116, reward function loss: -0.0019\n",
      "5450: reward:  35.00, mean_100:  50.26, episodes: 117, reward function loss: -0.0019\n",
      "5492: reward:  42.00, mean_100:  50.45, episodes: 118, reward function loss: -0.0019\n",
      "5528: reward:  36.00, mean_100:  50.65, episodes: 119, reward function loss: -0.0019\n",
      "5571: reward:  43.00, mean_100:  50.61, episodes: 120, reward function loss: -0.0020\n",
      "5604: reward:  33.00, mean_100:  50.61, episodes: 121, reward function loss: -0.0020\n",
      "5665: reward:  61.00, mean_100:  50.90, episodes: 122, reward function loss: -0.0020\n",
      "5706: reward:  41.00, mean_100:  50.86, episodes: 123, reward function loss: -0.0020\n",
      "5738: reward:  32.00, mean_100:  51.01, episodes: 124, reward function loss: -0.0017\n",
      "5791: reward:  53.00, mean_100:  50.87, episodes: 125, reward function loss: -0.0017\n",
      "5841: reward:  50.00, mean_100:  51.08, episodes: 126, reward function loss: -0.0017\n",
      "5894: reward:  53.00, mean_100:  51.38, episodes: 127, reward function loss: -0.0017\n",
      "5923: reward:  29.00, mean_100:  51.42, episodes: 128, reward function loss: -0.0023\n",
      "5990: reward:  67.00, mean_100:  51.87, episodes: 129, reward function loss: -0.0023\n",
      "6064: reward:  74.00, mean_100:  52.18, episodes: 130, reward function loss: -0.0023\n",
      "6111: reward:  47.00, mean_100:  52.33, episodes: 131, reward function loss: -0.0023\n",
      "6165: reward:  54.00, mean_100:  52.70, episodes: 132, reward function loss: -0.0030\n",
      "6199: reward:  34.00, mean_100:  52.31, episodes: 133, reward function loss: -0.0030\n",
      "6247: reward:  48.00, mean_100:  52.31, episodes: 134, reward function loss: -0.0030\n",
      "6304: reward:  57.00, mean_100:  52.14, episodes: 135, reward function loss: -0.0030\n",
      "6350: reward:  46.00, mean_100:  52.28, episodes: 136, reward function loss: -0.0021\n",
      "6432: reward:  82.00, mean_100:  52.28, episodes: 137, reward function loss: -0.0021\n",
      "6490: reward:  58.00, mean_100:  52.26, episodes: 138, reward function loss: -0.0021\n",
      "6546: reward:  56.00, mean_100:  52.57, episodes: 139, reward function loss: -0.0021\n",
      "6591: reward:  45.00, mean_100:  52.55, episodes: 140, reward function loss: -0.0023\n",
      "6668: reward:  77.00, mean_100:  52.67, episodes: 141, reward function loss: -0.0023\n",
      "6721: reward:  53.00, mean_100:  52.80, episodes: 142, reward function loss: -0.0023\n",
      "6753: reward:  32.00, mean_100:  52.62, episodes: 143, reward function loss: -0.0023\n",
      "6820: reward:  67.00, mean_100:  52.68, episodes: 144, reward function loss: -0.0025\n",
      "6878: reward:  58.00, mean_100:  52.97, episodes: 145, reward function loss: -0.0025\n",
      "6942: reward:  64.00, mean_100:  52.96, episodes: 146, reward function loss: -0.0025\n",
      "7031: reward:  89.00, mean_100:  53.49, episodes: 147, reward function loss: -0.0025\n",
      "7115: reward:  84.00, mean_100:  53.41, episodes: 148, reward function loss: -0.0039\n",
      "7225: reward: 110.00, mean_100:  54.34, episodes: 149, reward function loss: -0.0039\n",
      "7305: reward:  80.00, mean_100:  53.76, episodes: 150, reward function loss: -0.0039\n",
      "7385: reward:  80.00, mean_100:  54.14, episodes: 151, reward function loss: -0.0039\n",
      "7598: reward: 213.00, mean_100:  55.70, episodes: 152, reward function loss: -0.0082\n",
      "7727: reward: 129.00, mean_100:  55.99, episodes: 153, reward function loss: -0.0082\n",
      "7849: reward: 122.00, mean_100:  56.79, episodes: 154, reward function loss: -0.0082\n",
      "8027: reward: 178.00, mean_100:  58.08, episodes: 155, reward function loss: -0.0082\n",
      "8161: reward: 134.00, mean_100:  58.62, episodes: 156, reward function loss: -0.0063\n",
      "8274: reward: 113.00, mean_100:  59.29, episodes: 157, reward function loss: -0.0063\n",
      "8422: reward: 148.00, mean_100:  59.86, episodes: 158, reward function loss: -0.0063\n",
      "8656: reward: 234.00, mean_100:  61.05, episodes: 159, reward function loss: -0.0063\n",
      "8803: reward: 147.00, mean_100:  61.73, episodes: 160, reward function loss: -0.0084\n",
      "8937: reward: 134.00, mean_100:  61.97, episodes: 161, reward function loss: -0.0084\n",
      "9177: reward: 240.00, mean_100:  63.85, episodes: 162, reward function loss: -0.0084\n",
      "9313: reward: 136.00, mean_100:  64.81, episodes: 163, reward function loss: -0.0084\n",
      "9399: reward:  86.00, mean_100:  64.56, episodes: 164, reward function loss: -0.0090\n",
      "9534: reward: 135.00, mean_100:  65.09, episodes: 165, reward function loss: -0.0090\n",
      "9607: reward:  73.00, mean_100:  65.30, episodes: 166, reward function loss: -0.0090\n",
      "9883: reward: 276.00, mean_100:  67.61, episodes: 167, reward function loss: -0.0090\n",
      "9972: reward:  89.00, mean_100:  67.68, episodes: 168, reward function loss: -0.0056\n",
      "10092: reward: 120.00, mean_100:  68.38, episodes: 169, reward function loss: -0.0056\n",
      "10216: reward: 124.00, mean_100:  69.20, episodes: 170, reward function loss: -0.0056\n",
      "10277: reward:  61.00, mean_100:  69.49, episodes: 171, reward function loss: -0.0056\n",
      "10392: reward: 115.00, mean_100:  70.39, episodes: 172, reward function loss: -0.0061\n",
      "10435: reward:  43.00, mean_100:  70.48, episodes: 173, reward function loss: -0.0061\n",
      "10505: reward:  70.00, mean_100:  70.64, episodes: 174, reward function loss: -0.0061\n",
      "10622: reward: 117.00, mean_100:  70.91, episodes: 175, reward function loss: -0.0061\n",
      "10713: reward:  91.00, mean_100:  71.17, episodes: 176, reward function loss: -0.0052\n",
      "10807: reward:  94.00, mean_100:  71.85, episodes: 177, reward function loss: -0.0052\n",
      "10881: reward:  74.00, mean_100:  72.32, episodes: 178, reward function loss: -0.0052\n",
      "10961: reward:  80.00, mean_100:  72.13, episodes: 179, reward function loss: -0.0052\n",
      "11051: reward:  90.00, mean_100:  72.17, episodes: 180, reward function loss: -0.0068\n",
      "11137: reward:  86.00, mean_100:  72.59, episodes: 181, reward function loss: -0.0068\n",
      "11205: reward:  68.00, mean_100:  72.67, episodes: 182, reward function loss: -0.0068\n",
      "11273: reward:  68.00, mean_100:  73.04, episodes: 183, reward function loss: -0.0068\n",
      "11350: reward:  77.00, mean_100:  73.26, episodes: 184, reward function loss: -0.0062\n",
      "11408: reward:  58.00, mean_100:  73.27, episodes: 185, reward function loss: -0.0062\n",
      "11463: reward:  55.00, mean_100:  73.33, episodes: 186, reward function loss: -0.0062\n",
      "11550: reward:  87.00, mean_100:  73.88, episodes: 187, reward function loss: -0.0062\n",
      "11634: reward:  84.00, mean_100:  74.25, episodes: 188, reward function loss: -0.0066\n",
      "11731: reward:  97.00, mean_100:  74.57, episodes: 189, reward function loss: -0.0066\n",
      "11816: reward:  85.00, mean_100:  74.77, episodes: 190, reward function loss: -0.0066\n",
      "11877: reward:  61.00, mean_100:  74.88, episodes: 191, reward function loss: -0.0066\n",
      "11974: reward:  97.00, mean_100:  75.58, episodes: 192, reward function loss: -0.0063\n",
      "12020: reward:  46.00, mean_100:  75.76, episodes: 193, reward function loss: -0.0063\n",
      "12069: reward:  49.00, mean_100:  75.75, episodes: 194, reward function loss: -0.0063\n",
      "12144: reward:  75.00, mean_100:  76.02, episodes: 195, reward function loss: -0.0063\n",
      "12213: reward:  69.00, mean_100:  76.30, episodes: 196, reward function loss: -0.0053\n",
      "12272: reward:  59.00, mean_100:  76.53, episodes: 197, reward function loss: -0.0053\n",
      "12332: reward:  60.00, mean_100:  76.85, episodes: 198, reward function loss: -0.0053\n",
      "12369: reward:  37.00, mean_100:  76.76, episodes: 199, reward function loss: -0.0053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12437: reward:  68.00, mean_100:  77.03, episodes: 200, reward function loss: -0.0047\n",
      "12488: reward:  51.00, mean_100:  77.26, episodes: 201, reward function loss: -0.0047\n",
      "12547: reward:  59.00, mean_100:  77.51, episodes: 202, reward function loss: -0.0047\n",
      "12618: reward:  71.00, mean_100:  77.60, episodes: 203, reward function loss: -0.0047\n",
      "12676: reward:  58.00, mean_100:  77.50, episodes: 204, reward function loss: -0.0053\n",
      "12741: reward:  65.00, mean_100:  77.88, episodes: 205, reward function loss: -0.0053\n",
      "12812: reward:  71.00, mean_100:  78.30, episodes: 206, reward function loss: -0.0053\n",
      "12882: reward:  70.00, mean_100:  78.69, episodes: 207, reward function loss: -0.0053\n",
      "12947: reward:  65.00, mean_100:  78.64, episodes: 208, reward function loss: -0.0059\n",
      "13015: reward:  68.00, mean_100:  78.70, episodes: 209, reward function loss: -0.0059\n",
      "13062: reward:  47.00, mean_100:  78.95, episodes: 210, reward function loss: -0.0059\n",
      "13095: reward:  33.00, mean_100:  78.89, episodes: 211, reward function loss: -0.0059\n",
      "13188: reward:  93.00, mean_100:  79.46, episodes: 212, reward function loss: -0.0053\n",
      "13223: reward:  35.00, mean_100:  79.40, episodes: 213, reward function loss: -0.0053\n",
      "13282: reward:  59.00, mean_100:  79.49, episodes: 214, reward function loss: -0.0053\n",
      "13346: reward:  64.00, mean_100:  79.67, episodes: 215, reward function loss: -0.0053\n",
      "13418: reward:  72.00, mean_100:  80.03, episodes: 216, reward function loss: -0.0053\n",
      "13458: reward:  40.00, mean_100:  80.08, episodes: 217, reward function loss: -0.0053\n",
      "13539: reward:  81.00, mean_100:  80.47, episodes: 218, reward function loss: -0.0053\n",
      "13633: reward:  94.00, mean_100:  81.05, episodes: 219, reward function loss: -0.0053\n",
      "13691: reward:  58.00, mean_100:  81.20, episodes: 220, reward function loss: -0.0061\n",
      "13750: reward:  59.00, mean_100:  81.46, episodes: 221, reward function loss: -0.0061\n",
      "13795: reward:  45.00, mean_100:  81.30, episodes: 222, reward function loss: -0.0061\n",
      "13851: reward:  56.00, mean_100:  81.45, episodes: 223, reward function loss: -0.0061\n",
      "13909: reward:  58.00, mean_100:  81.71, episodes: 224, reward function loss: -0.0051\n",
      "13949: reward:  40.00, mean_100:  81.58, episodes: 225, reward function loss: -0.0051\n",
      "14036: reward:  87.00, mean_100:  81.95, episodes: 226, reward function loss: -0.0051\n",
      "14136: reward: 100.00, mean_100:  82.42, episodes: 227, reward function loss: -0.0051\n",
      "14209: reward:  73.00, mean_100:  82.86, episodes: 228, reward function loss: -0.0057\n",
      "14271: reward:  62.00, mean_100:  82.81, episodes: 229, reward function loss: -0.0057\n",
      "14378: reward: 107.00, mean_100:  83.14, episodes: 230, reward function loss: -0.0057\n",
      "14434: reward:  56.00, mean_100:  83.23, episodes: 231, reward function loss: -0.0057\n",
      "14504: reward:  70.00, mean_100:  83.39, episodes: 232, reward function loss: -0.0065\n",
      "14595: reward:  91.00, mean_100:  83.96, episodes: 233, reward function loss: -0.0065\n",
      "14650: reward:  55.00, mean_100:  84.03, episodes: 234, reward function loss: -0.0065\n",
      "14723: reward:  73.00, mean_100:  84.19, episodes: 235, reward function loss: -0.0065\n",
      "14812: reward:  89.00, mean_100:  84.62, episodes: 236, reward function loss: -0.0069\n",
      "14870: reward:  58.00, mean_100:  84.38, episodes: 237, reward function loss: -0.0069\n",
      "14926: reward:  56.00, mean_100:  84.36, episodes: 238, reward function loss: -0.0069\n",
      "15002: reward:  76.00, mean_100:  84.56, episodes: 239, reward function loss: -0.0069\n",
      "15057: reward:  55.00, mean_100:  84.66, episodes: 240, reward function loss: -0.0056\n",
      "15158: reward: 101.00, mean_100:  84.90, episodes: 241, reward function loss: -0.0056\n",
      "15237: reward:  79.00, mean_100:  85.16, episodes: 242, reward function loss: -0.0056\n",
      "15329: reward:  92.00, mean_100:  85.76, episodes: 243, reward function loss: -0.0056\n",
      "15459: reward: 130.00, mean_100:  86.39, episodes: 244, reward function loss: -0.0089\n",
      "15537: reward:  78.00, mean_100:  86.59, episodes: 245, reward function loss: -0.0089\n",
      "15610: reward:  73.00, mean_100:  86.68, episodes: 246, reward function loss: -0.0089\n",
      "15746: reward: 136.00, mean_100:  87.15, episodes: 247, reward function loss: -0.0089\n",
      "15854: reward: 108.00, mean_100:  87.39, episodes: 248, reward function loss: -0.0082\n",
      "15961: reward: 107.00, mean_100:  87.36, episodes: 249, reward function loss: -0.0082\n",
      "16036: reward:  75.00, mean_100:  87.31, episodes: 250, reward function loss: -0.0082\n",
      "16098: reward:  62.00, mean_100:  87.13, episodes: 251, reward function loss: -0.0082\n",
      "16213: reward: 115.00, mean_100:  86.15, episodes: 252, reward function loss: -0.0087\n",
      "16347: reward: 134.00, mean_100:  86.20, episodes: 253, reward function loss: -0.0087\n",
      "16481: reward: 134.00, mean_100:  86.32, episodes: 254, reward function loss: -0.0087\n",
      "16593: reward: 112.00, mean_100:  85.66, episodes: 255, reward function loss: -0.0087\n",
      "16686: reward:  93.00, mean_100:  85.25, episodes: 256, reward function loss: -0.0103\n",
      "16824: reward: 138.00, mean_100:  85.50, episodes: 257, reward function loss: -0.0103\n",
      "16943: reward: 119.00, mean_100:  85.21, episodes: 258, reward function loss: -0.0103\n",
      "17060: reward: 117.00, mean_100:  84.04, episodes: 259, reward function loss: -0.0103\n",
      "17180: reward: 120.00, mean_100:  83.77, episodes: 260, reward function loss: -0.0116\n",
      "17455: reward: 275.00, mean_100:  85.18, episodes: 261, reward function loss: -0.0116\n",
      "17653: reward: 198.00, mean_100:  84.76, episodes: 262, reward function loss: -0.0116\n",
      "17887: reward: 234.00, mean_100:  85.74, episodes: 263, reward function loss: -0.0116\n",
      "18085: reward: 198.00, mean_100:  86.86, episodes: 264, reward function loss: -0.0200\n",
      "18220: reward: 135.00, mean_100:  86.86, episodes: 265, reward function loss: -0.0200\n",
      "18328: reward: 108.00, mean_100:  87.21, episodes: 266, reward function loss: -0.0200\n",
      "18421: reward:  93.00, mean_100:  85.38, episodes: 267, reward function loss: -0.0200\n",
      "18513: reward:  92.00, mean_100:  85.41, episodes: 268, reward function loss: -0.0101\n",
      "19013: reward: 500.00, mean_100:  89.21, episodes: 269, reward function loss: -0.0101\n",
      "19513: reward: 500.00, mean_100:  92.97, episodes: 270, reward function loss: -0.0101\n",
      "19773: reward: 260.00, mean_100:  94.96, episodes: 271, reward function loss: -0.0101\n",
      "20070: reward: 297.00, mean_100:  96.78, episodes: 272, reward function loss: -0.0144\n",
      "20410: reward: 340.00, mean_100:  99.75, episodes: 273, reward function loss: -0.0144\n",
      "20910: reward: 500.00, mean_100: 104.05, episodes: 274, reward function loss: -0.0144\n",
      "21070: reward: 160.00, mean_100: 104.48, episodes: 275, reward function loss: -0.0144\n",
      "21409: reward: 339.00, mean_100: 106.96, episodes: 276, reward function loss: -0.0078\n",
      "21892: reward: 483.00, mean_100: 110.85, episodes: 277, reward function loss: -0.0078\n",
      "22226: reward: 334.00, mean_100: 113.45, episodes: 278, reward function loss: -0.0078\n",
      "22490: reward: 264.00, mean_100: 115.29, episodes: 279, reward function loss: -0.0078\n",
      "22809: reward: 319.00, mean_100: 117.58, episodes: 280, reward function loss: -0.0182\n",
      "23017: reward: 208.00, mean_100: 118.80, episodes: 281, reward function loss: -0.0182\n",
      "23402: reward: 385.00, mean_100: 121.97, episodes: 282, reward function loss: -0.0182\n",
      "23591: reward: 189.00, mean_100: 123.18, episodes: 283, reward function loss: -0.0182\n",
      "23841: reward: 250.00, mean_100: 124.91, episodes: 284, reward function loss: -0.0157\n",
      "24111: reward: 270.00, mean_100: 127.03, episodes: 285, reward function loss: -0.0157\n",
      "24314: reward: 203.00, mean_100: 128.51, episodes: 286, reward function loss: -0.0157\n",
      "24479: reward: 165.00, mean_100: 129.29, episodes: 287, reward function loss: -0.0157\n",
      "24683: reward: 204.00, mean_100: 130.49, episodes: 288, reward function loss: -0.0155\n",
      "24860: reward: 177.00, mean_100: 131.29, episodes: 289, reward function loss: -0.0155\n",
      "25043: reward: 183.00, mean_100: 132.27, episodes: 290, reward function loss: -0.0155\n",
      "25289: reward: 246.00, mean_100: 134.12, episodes: 291, reward function loss: -0.0155\n",
      "25496: reward: 207.00, mean_100: 135.22, episodes: 292, reward function loss: -0.0097\n",
      "25722: reward: 226.00, mean_100: 137.02, episodes: 293, reward function loss: -0.0097\n",
      "25946: reward: 224.00, mean_100: 138.77, episodes: 294, reward function loss: -0.0097\n",
      "26191: reward: 245.00, mean_100: 140.47, episodes: 295, reward function loss: -0.0097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26435: reward: 244.00, mean_100: 142.22, episodes: 296, reward function loss: -0.0136\n",
      "26665: reward: 230.00, mean_100: 143.93, episodes: 297, reward function loss: -0.0136\n",
      "26889: reward: 224.00, mean_100: 145.57, episodes: 298, reward function loss: -0.0136\n",
      "27077: reward: 188.00, mean_100: 147.08, episodes: 299, reward function loss: -0.0136\n",
      "27245: reward: 168.00, mean_100: 148.08, episodes: 300, reward function loss: -0.0146\n",
      "27479: reward: 234.00, mean_100: 149.91, episodes: 301, reward function loss: -0.0146\n",
      "27690: reward: 211.00, mean_100: 151.43, episodes: 302, reward function loss: -0.0146\n",
      "27888: reward: 198.00, mean_100: 152.70, episodes: 303, reward function loss: -0.0146\n",
      "28117: reward: 229.00, mean_100: 154.41, episodes: 304, reward function loss: -0.0160\n",
      "28347: reward: 230.00, mean_100: 156.06, episodes: 305, reward function loss: -0.0160\n",
      "28584: reward: 237.00, mean_100: 157.72, episodes: 306, reward function loss: -0.0160\n",
      "28839: reward: 255.00, mean_100: 159.57, episodes: 307, reward function loss: -0.0160\n",
      "29038: reward: 199.00, mean_100: 160.91, episodes: 308, reward function loss: -0.0164\n",
      "29238: reward: 200.00, mean_100: 162.23, episodes: 309, reward function loss: -0.0164\n",
      "29491: reward: 253.00, mean_100: 164.29, episodes: 310, reward function loss: -0.0164\n",
      "29753: reward: 262.00, mean_100: 166.58, episodes: 311, reward function loss: -0.0164\n",
      "30026: reward: 273.00, mean_100: 168.38, episodes: 312, reward function loss: -0.0181\n",
      "30255: reward: 229.00, mean_100: 170.32, episodes: 313, reward function loss: -0.0181\n",
      "30539: reward: 284.00, mean_100: 172.57, episodes: 314, reward function loss: -0.0181\n",
      "30776: reward: 237.00, mean_100: 174.30, episodes: 315, reward function loss: -0.0181\n",
      "30989: reward: 213.00, mean_100: 175.71, episodes: 316, reward function loss: -0.0184\n",
      "31268: reward: 279.00, mean_100: 178.10, episodes: 317, reward function loss: -0.0184\n",
      "31545: reward: 277.00, mean_100: 180.06, episodes: 318, reward function loss: -0.0184\n",
      "31877: reward: 332.00, mean_100: 182.44, episodes: 319, reward function loss: -0.0184\n",
      "32162: reward: 285.00, mean_100: 184.71, episodes: 320, reward function loss: -0.0221\n",
      "32477: reward: 315.00, mean_100: 187.27, episodes: 321, reward function loss: -0.0221\n",
      "32809: reward: 332.00, mean_100: 190.14, episodes: 322, reward function loss: -0.0221\n",
      "33122: reward: 313.00, mean_100: 192.71, episodes: 323, reward function loss: -0.0221\n",
      "33327: reward: 205.00, mean_100: 194.18, episodes: 324, reward function loss: -0.0212\n",
      "33658: reward: 331.00, mean_100: 197.09, episodes: 325, reward function loss: -0.0212\n",
      "34012: reward: 354.00, mean_100: 199.76, episodes: 326, reward function loss: -0.0212\n",
      "34239: reward: 227.00, mean_100: 201.03, episodes: 327, reward function loss: -0.0212\n",
      "34453: reward: 214.00, mean_100: 202.44, episodes: 328, reward function loss: -0.0190\n",
      "34905: reward: 452.00, mean_100: 206.34, episodes: 329, reward function loss: -0.0190\n",
      "35405: reward: 500.00, mean_100: 210.27, episodes: 330, reward function loss: -0.0190\n",
      "35718: reward: 313.00, mean_100: 212.84, episodes: 331, reward function loss: -0.0190\n",
      "36218: reward: 500.00, mean_100: 217.14, episodes: 332, reward function loss: -0.0347\n",
      "36718: reward: 500.00, mean_100: 221.23, episodes: 333, reward function loss: -0.0347\n",
      "36950: reward: 232.00, mean_100: 223.00, episodes: 334, reward function loss: -0.0347\n",
      "37245: reward: 295.00, mean_100: 225.22, episodes: 335, reward function loss: -0.0347\n",
      "37745: reward: 500.00, mean_100: 229.33, episodes: 336, reward function loss: -0.0231\n",
      "38245: reward: 500.00, mean_100: 233.75, episodes: 337, reward function loss: -0.0231\n",
      "38649: reward: 404.00, mean_100: 237.23, episodes: 338, reward function loss: -0.0231\n",
      "38908: reward: 259.00, mean_100: 239.06, episodes: 339, reward function loss: -0.0231\n",
      "39281: reward: 373.00, mean_100: 242.24, episodes: 340, reward function loss: -0.0227\n",
      "39781: reward: 500.00, mean_100: 246.23, episodes: 341, reward function loss: -0.0227\n",
      "40117: reward: 336.00, mean_100: 248.80, episodes: 342, reward function loss: -0.0227\n",
      "40617: reward: 500.00, mean_100: 252.88, episodes: 343, reward function loss: -0.0227\n",
      "41107: reward: 490.00, mean_100: 256.48, episodes: 344, reward function loss: -0.0168\n",
      "41585: reward: 478.00, mean_100: 260.48, episodes: 345, reward function loss: -0.0168\n",
      "42085: reward: 500.00, mean_100: 264.75, episodes: 346, reward function loss: -0.0168\n",
      "42585: reward: 500.00, mean_100: 268.39, episodes: 347, reward function loss: -0.0168\n",
      "42946: reward: 361.00, mean_100: 270.92, episodes: 348, reward function loss: -0.0011\n",
      "43446: reward: 500.00, mean_100: 274.85, episodes: 349, reward function loss: -0.0011\n",
      "43946: reward: 500.00, mean_100: 279.10, episodes: 350, reward function loss: -0.0011\n",
      "44446: reward: 500.00, mean_100: 283.48, episodes: 351, reward function loss: -0.0011\n",
      "44946: reward: 500.00, mean_100: 287.33, episodes: 352, reward function loss: -0.0195\n",
      "45446: reward: 500.00, mean_100: 290.99, episodes: 353, reward function loss: -0.0195\n",
      "45946: reward: 500.00, mean_100: 294.65, episodes: 354, reward function loss: -0.0195\n",
      "46446: reward: 500.00, mean_100: 298.53, episodes: 355, reward function loss: -0.0195\n",
      "46946: reward: 500.00, mean_100: 302.60, episodes: 356, reward function loss: -0.0285\n",
      "47446: reward: 500.00, mean_100: 306.22, episodes: 357, reward function loss: -0.0285\n",
      "47946: reward: 500.00, mean_100: 310.03, episodes: 358, reward function loss: -0.0285\n",
      "48446: reward: 500.00, mean_100: 313.86, episodes: 359, reward function loss: -0.0285\n",
      "48946: reward: 500.00, mean_100: 317.66, episodes: 360, reward function loss: -0.0387\n",
      "49446: reward: 500.00, mean_100: 319.91, episodes: 361, reward function loss: -0.0387\n",
      "49946: reward: 500.00, mean_100: 322.93, episodes: 362, reward function loss: -0.0387\n",
      "50394: reward: 448.00, mean_100: 325.07, episodes: 363, reward function loss: -0.0387\n",
      "50894: reward: 500.00, mean_100: 328.09, episodes: 364, reward function loss: -0.0411\n",
      "51394: reward: 500.00, mean_100: 331.74, episodes: 365, reward function loss: -0.0411\n",
      "51894: reward: 500.00, mean_100: 335.66, episodes: 366, reward function loss: -0.0411\n",
      "52394: reward: 500.00, mean_100: 339.73, episodes: 367, reward function loss: -0.0411\n",
      "52894: reward: 500.00, mean_100: 343.81, episodes: 368, reward function loss: -0.0454\n",
      "53394: reward: 500.00, mean_100: 343.81, episodes: 369, reward function loss: -0.0454\n",
      "53894: reward: 500.00, mean_100: 343.81, episodes: 370, reward function loss: -0.0454\n",
      "54394: reward: 500.00, mean_100: 346.21, episodes: 371, reward function loss: -0.0454\n",
      "54894: reward: 500.00, mean_100: 348.24, episodes: 372, reward function loss: -0.0399\n",
      "55394: reward: 500.00, mean_100: 349.84, episodes: 373, reward function loss: -0.0399\n",
      "55894: reward: 500.00, mean_100: 349.84, episodes: 374, reward function loss: -0.0399\n",
      "56394: reward: 500.00, mean_100: 353.24, episodes: 375, reward function loss: -0.0399\n",
      "56845: reward: 451.00, mean_100: 354.36, episodes: 376, reward function loss: -0.0391\n",
      "56905: reward:  60.00, mean_100: 350.13, episodes: 377, reward function loss: -0.0391\n",
      "57405: reward: 500.00, mean_100: 351.79, episodes: 378, reward function loss: -0.0391\n",
      "57630: reward: 225.00, mean_100: 351.40, episodes: 379, reward function loss: -0.0391\n",
      "58130: reward: 500.00, mean_100: 353.21, episodes: 380, reward function loss: -0.0285\n",
      "58630: reward: 500.00, mean_100: 356.13, episodes: 381, reward function loss: -0.0285\n",
      "59130: reward: 500.00, mean_100: 357.28, episodes: 382, reward function loss: -0.0285\n",
      "59630: reward: 500.00, mean_100: 360.39, episodes: 383, reward function loss: -0.0285\n",
      "60130: reward: 500.00, mean_100: 362.89, episodes: 384, reward function loss: -0.0407\n",
      "60630: reward: 500.00, mean_100: 365.19, episodes: 385, reward function loss: -0.0407\n",
      "61130: reward: 500.00, mean_100: 368.16, episodes: 386, reward function loss: -0.0407\n",
      "61630: reward: 500.00, mean_100: 371.51, episodes: 387, reward function loss: -0.0407\n",
      "62130: reward: 500.00, mean_100: 374.47, episodes: 388, reward function loss: -0.0358\n",
      "62630: reward: 500.00, mean_100: 377.70, episodes: 389, reward function loss: -0.0358\n",
      "62967: reward: 337.00, mean_100: 379.24, episodes: 390, reward function loss: -0.0358\n",
      "63408: reward: 441.00, mean_100: 381.19, episodes: 391, reward function loss: -0.0358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63824: reward: 416.00, mean_100: 383.28, episodes: 392, reward function loss: -0.0296\n",
      "64324: reward: 500.00, mean_100: 386.02, episodes: 393, reward function loss: -0.0296\n",
      "64713: reward: 389.00, mean_100: 387.67, episodes: 394, reward function loss: -0.0296\n",
      "65213: reward: 500.00, mean_100: 390.22, episodes: 395, reward function loss: -0.0296\n",
      "65713: reward: 500.00, mean_100: 392.78, episodes: 396, reward function loss: -0.0394\n",
      "66213: reward: 500.00, mean_100: 395.48, episodes: 397, reward function loss: -0.0394\n",
      "66532: reward: 319.00, mean_100: 396.43, episodes: 398, reward function loss: -0.0394\n",
      "67032: reward: 500.00, mean_100: 399.55, episodes: 399, reward function loss: -0.0394\n",
      "67119: reward:  87.00, mean_100: 398.74, episodes: 400, reward function loss: -0.0098\n",
      "67512: reward: 393.00, mean_100: 400.33, episodes: 401, reward function loss: -0.0098\n",
      "68012: reward: 500.00, mean_100: 403.22, episodes: 402, reward function loss: -0.0098\n",
      "68233: reward: 221.00, mean_100: 403.45, episodes: 403, reward function loss: -0.0098\n",
      "68395: reward: 162.00, mean_100: 402.78, episodes: 404, reward function loss: -0.0119\n",
      "68576: reward: 181.00, mean_100: 402.29, episodes: 405, reward function loss: -0.0119\n",
      "69076: reward: 500.00, mean_100: 404.92, episodes: 406, reward function loss: -0.0119\n",
      "69354: reward: 278.00, mean_100: 405.15, episodes: 407, reward function loss: -0.0119\n",
      "69591: reward: 237.00, mean_100: 405.53, episodes: 408, reward function loss: -0.0070\n",
      "70031: reward: 440.00, mean_100: 407.93, episodes: 409, reward function loss: -0.0070\n",
      "70531: reward: 500.00, mean_100: 410.40, episodes: 410, reward function loss: -0.0070\n",
      "70693: reward: 162.00, mean_100: 409.40, episodes: 411, reward function loss: -0.0070\n",
      "70953: reward: 260.00, mean_100: 409.27, episodes: 412, reward function loss: -0.0068\n",
      "71100: reward: 147.00, mean_100: 408.45, episodes: 413, reward function loss: -0.0068\n",
      "71333: reward: 233.00, mean_100: 407.94, episodes: 414, reward function loss: -0.0068\n",
      "71540: reward: 207.00, mean_100: 407.64, episodes: 415, reward function loss: -0.0068\n",
      "72001: reward: 461.00, mean_100: 410.12, episodes: 416, reward function loss: -0.0044\n",
      "72487: reward: 486.00, mean_100: 412.19, episodes: 417, reward function loss: -0.0044\n",
      "72987: reward: 500.00, mean_100: 414.42, episodes: 418, reward function loss: -0.0044\n",
      "73373: reward: 386.00, mean_100: 414.96, episodes: 419, reward function loss: -0.0044\n",
      "73661: reward: 288.00, mean_100: 414.99, episodes: 420, reward function loss: -0.0068\n",
      "74161: reward: 500.00, mean_100: 416.84, episodes: 421, reward function loss: -0.0068\n",
      "74661: reward: 500.00, mean_100: 418.52, episodes: 422, reward function loss: -0.0068\n",
      "75161: reward: 500.00, mean_100: 420.39, episodes: 423, reward function loss: -0.0068\n",
      "75661: reward: 500.00, mean_100: 423.34, episodes: 424, reward function loss: -0.0078\n",
      "76087: reward: 426.00, mean_100: 424.29, episodes: 425, reward function loss: -0.0078\n",
      "76300: reward: 213.00, mean_100: 422.88, episodes: 426, reward function loss: -0.0078\n",
      "76773: reward: 473.00, mean_100: 425.34, episodes: 427, reward function loss: -0.0078\n",
      "77273: reward: 500.00, mean_100: 428.20, episodes: 428, reward function loss: -0.0102\n",
      "77773: reward: 500.00, mean_100: 428.68, episodes: 429, reward function loss: -0.0102\n",
      "77960: reward: 187.00, mean_100: 425.55, episodes: 430, reward function loss: -0.0102\n",
      "78396: reward: 436.00, mean_100: 426.78, episodes: 431, reward function loss: -0.0102\n",
      "78896: reward: 500.00, mean_100: 426.78, episodes: 432, reward function loss: -0.0075\n",
      "79396: reward: 500.00, mean_100: 426.78, episodes: 433, reward function loss: -0.0075\n",
      "79896: reward: 500.00, mean_100: 429.46, episodes: 434, reward function loss: -0.0075\n",
      "80396: reward: 500.00, mean_100: 431.51, episodes: 435, reward function loss: -0.0075\n",
      "80554: reward: 158.00, mean_100: 428.09, episodes: 436, reward function loss: -0.0116\n",
      "81054: reward: 500.00, mean_100: 428.09, episodes: 437, reward function loss: -0.0116\n",
      "81554: reward: 500.00, mean_100: 429.05, episodes: 438, reward function loss: -0.0116\n",
      "82054: reward: 500.00, mean_100: 431.46, episodes: 439, reward function loss: -0.0116\n",
      "82554: reward: 500.00, mean_100: 432.73, episodes: 440, reward function loss: -0.0077\n",
      "83054: reward: 500.00, mean_100: 432.73, episodes: 441, reward function loss: -0.0077\n",
      "83554: reward: 500.00, mean_100: 434.37, episodes: 442, reward function loss: -0.0077\n",
      "84054: reward: 500.00, mean_100: 434.37, episodes: 443, reward function loss: -0.0077\n",
      "84554: reward: 500.00, mean_100: 434.47, episodes: 444, reward function loss: -0.0166\n",
      "85054: reward: 500.00, mean_100: 434.69, episodes: 445, reward function loss: -0.0166\n",
      "85554: reward: 500.00, mean_100: 434.69, episodes: 446, reward function loss: -0.0166\n",
      "86054: reward: 500.00, mean_100: 434.69, episodes: 447, reward function loss: -0.0166\n",
      "86554: reward: 500.00, mean_100: 436.08, episodes: 448, reward function loss: -0.0183\n",
      "87054: reward: 500.00, mean_100: 436.08, episodes: 449, reward function loss: -0.0183\n",
      "87554: reward: 500.00, mean_100: 436.08, episodes: 450, reward function loss: -0.0183\n",
      "88054: reward: 500.00, mean_100: 436.08, episodes: 451, reward function loss: -0.0183\n",
      "88554: reward: 500.00, mean_100: 436.08, episodes: 452, reward function loss: -0.0137\n",
      "89054: reward: 500.00, mean_100: 436.08, episodes: 453, reward function loss: -0.0137\n",
      "89554: reward: 500.00, mean_100: 436.08, episodes: 454, reward function loss: -0.0137\n",
      "90054: reward: 500.00, mean_100: 436.08, episodes: 455, reward function loss: -0.0137\n",
      "90554: reward: 500.00, mean_100: 436.08, episodes: 456, reward function loss: -0.0189\n",
      "91054: reward: 500.00, mean_100: 436.08, episodes: 457, reward function loss: -0.0189\n",
      "91554: reward: 500.00, mean_100: 436.08, episodes: 458, reward function loss: -0.0189\n",
      "92054: reward: 500.00, mean_100: 436.08, episodes: 459, reward function loss: -0.0189\n",
      "92554: reward: 500.00, mean_100: 436.08, episodes: 460, reward function loss: -0.0166\n",
      "93054: reward: 500.00, mean_100: 436.08, episodes: 461, reward function loss: -0.0166\n",
      "93554: reward: 500.00, mean_100: 436.08, episodes: 462, reward function loss: -0.0166\n",
      "94054: reward: 500.00, mean_100: 436.60, episodes: 463, reward function loss: -0.0166\n",
      "94554: reward: 500.00, mean_100: 436.60, episodes: 464, reward function loss: -0.0036\n",
      "95054: reward: 500.00, mean_100: 436.60, episodes: 465, reward function loss: -0.0036\n",
      "95554: reward: 500.00, mean_100: 436.60, episodes: 466, reward function loss: -0.0036\n",
      "96054: reward: 500.00, mean_100: 436.60, episodes: 467, reward function loss: -0.0036\n",
      "96554: reward: 500.00, mean_100: 436.60, episodes: 468, reward function loss: -0.0057\n",
      "97054: reward: 500.00, mean_100: 436.60, episodes: 469, reward function loss: -0.0057\n",
      "97554: reward: 500.00, mean_100: 436.60, episodes: 470, reward function loss: -0.0057\n",
      "98054: reward: 500.00, mean_100: 436.60, episodes: 471, reward function loss: -0.0057\n",
      "98554: reward: 500.00, mean_100: 436.60, episodes: 472, reward function loss: -0.0001\n",
      "99054: reward: 500.00, mean_100: 436.60, episodes: 473, reward function loss: -0.0001\n",
      "99554: reward: 500.00, mean_100: 436.60, episodes: 474, reward function loss: -0.0001\n",
      "100054: reward: 500.00, mean_100: 436.60, episodes: 475, reward function loss: -0.0001\n",
      "100554: reward: 500.00, mean_100: 437.09, episodes: 476, reward function loss: -0.0000\n",
      "101054: reward: 500.00, mean_100: 441.49, episodes: 477, reward function loss: -0.0000\n",
      "101554: reward: 500.00, mean_100: 441.49, episodes: 478, reward function loss: -0.0000\n",
      "102054: reward: 500.00, mean_100: 444.24, episodes: 479, reward function loss: -0.0000\n",
      "102554: reward: 500.00, mean_100: 444.24, episodes: 480, reward function loss: 0.0000\n",
      "103054: reward: 500.00, mean_100: 444.24, episodes: 481, reward function loss: 0.0000\n",
      "103554: reward: 500.00, mean_100: 444.24, episodes: 482, reward function loss: 0.0000\n",
      "104054: reward: 500.00, mean_100: 444.24, episodes: 483, reward function loss: 0.0000\n",
      "104554: reward: 500.00, mean_100: 444.24, episodes: 484, reward function loss: 0.0000\n",
      "105054: reward: 500.00, mean_100: 444.24, episodes: 485, reward function loss: 0.0000\n",
      "105554: reward: 500.00, mean_100: 444.24, episodes: 486, reward function loss: 0.0000\n",
      "106054: reward: 500.00, mean_100: 444.24, episodes: 487, reward function loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106554: reward: 500.00, mean_100: 444.24, episodes: 488, reward function loss: 0.0000\n",
      "107054: reward: 500.00, mean_100: 444.24, episodes: 489, reward function loss: 0.0000\n",
      "107554: reward: 500.00, mean_100: 445.87, episodes: 490, reward function loss: 0.0000\n",
      "108054: reward: 500.00, mean_100: 446.46, episodes: 491, reward function loss: 0.0000\n",
      "108554: reward: 500.00, mean_100: 447.30, episodes: 492, reward function loss: -0.0000\n",
      "109054: reward: 500.00, mean_100: 447.30, episodes: 493, reward function loss: -0.0000\n",
      "109554: reward: 500.00, mean_100: 448.41, episodes: 494, reward function loss: -0.0000\n",
      "110054: reward: 500.00, mean_100: 448.41, episodes: 495, reward function loss: -0.0000\n",
      "110554: reward: 500.00, mean_100: 448.41, episodes: 496, reward function loss: -0.0131\n",
      "111054: reward: 500.00, mean_100: 448.41, episodes: 497, reward function loss: -0.0131\n",
      "111554: reward: 500.00, mean_100: 450.22, episodes: 498, reward function loss: -0.0131\n",
      "112054: reward: 500.00, mean_100: 450.22, episodes: 499, reward function loss: -0.0131\n",
      "112554: reward: 500.00, mean_100: 454.35, episodes: 500, reward function loss: -0.0275\n",
      "113054: reward: 500.00, mean_100: 455.42, episodes: 501, reward function loss: -0.0275\n",
      "113554: reward: 500.00, mean_100: 455.42, episodes: 502, reward function loss: -0.0275\n",
      "114054: reward: 500.00, mean_100: 458.21, episodes: 503, reward function loss: -0.0275\n",
      "114554: reward: 500.00, mean_100: 461.59, episodes: 504, reward function loss: -0.0390\n",
      "115054: reward: 500.00, mean_100: 464.78, episodes: 505, reward function loss: -0.0390\n",
      "115554: reward: 500.00, mean_100: 464.78, episodes: 506, reward function loss: -0.0390\n",
      "116054: reward: 500.00, mean_100: 467.00, episodes: 507, reward function loss: -0.0390\n",
      "116554: reward: 500.00, mean_100: 469.63, episodes: 508, reward function loss: -0.0414\n",
      "117054: reward: 500.00, mean_100: 470.23, episodes: 509, reward function loss: -0.0414\n",
      "117554: reward: 500.00, mean_100: 470.23, episodes: 510, reward function loss: -0.0414\n",
      "118054: reward: 500.00, mean_100: 473.61, episodes: 511, reward function loss: -0.0414\n",
      "118554: reward: 500.00, mean_100: 476.01, episodes: 512, reward function loss: -0.0363\n",
      "119054: reward: 500.00, mean_100: 479.54, episodes: 513, reward function loss: -0.0363\n",
      "119554: reward: 500.00, mean_100: 482.21, episodes: 514, reward function loss: -0.0363\n",
      "120054: reward: 500.00, mean_100: 485.14, episodes: 515, reward function loss: -0.0363\n",
      "120554: reward: 500.00, mean_100: 485.53, episodes: 516, reward function loss: -0.0390\n",
      "121054: reward: 500.00, mean_100: 485.67, episodes: 517, reward function loss: -0.0390\n",
      "121554: reward: 500.00, mean_100: 485.67, episodes: 518, reward function loss: -0.0390\n",
      "122054: reward: 500.00, mean_100: 486.81, episodes: 519, reward function loss: -0.0390\n",
      "122554: reward: 500.00, mean_100: 488.93, episodes: 520, reward function loss: -0.0392\n",
      "123054: reward: 500.00, mean_100: 488.93, episodes: 521, reward function loss: -0.0392\n",
      "123554: reward: 500.00, mean_100: 488.93, episodes: 522, reward function loss: -0.0392\n",
      "124054: reward: 500.00, mean_100: 488.93, episodes: 523, reward function loss: -0.0392\n",
      "124554: reward: 500.00, mean_100: 488.93, episodes: 524, reward function loss: -0.0400\n",
      "125054: reward: 500.00, mean_100: 489.67, episodes: 525, reward function loss: -0.0400\n",
      "125554: reward: 500.00, mean_100: 492.54, episodes: 526, reward function loss: -0.0400\n",
      "126054: reward: 500.00, mean_100: 492.81, episodes: 527, reward function loss: -0.0400\n",
      "126554: reward: 500.00, mean_100: 492.81, episodes: 528, reward function loss: -0.0392\n",
      "127054: reward: 500.00, mean_100: 492.81, episodes: 529, reward function loss: -0.0392\n",
      "127554: reward: 500.00, mean_100: 495.94, episodes: 530, reward function loss: -0.0392\n",
      "128054: reward: 500.00, mean_100: 496.58, episodes: 531, reward function loss: -0.0392\n",
      "128554: reward: 500.00, mean_100: 496.58, episodes: 532, reward function loss: -0.0401\n",
      "129054: reward: 500.00, mean_100: 496.58, episodes: 533, reward function loss: -0.0401\n",
      "129554: reward: 500.00, mean_100: 496.58, episodes: 534, reward function loss: -0.0401\n",
      "130054: reward: 500.00, mean_100: 496.58, episodes: 535, reward function loss: -0.0401\n",
      "130554: reward: 500.00, mean_100: 500.00, episodes: 536, reward function loss: -0.0351\n",
      "Solved in 130554 steps and 536 episodes!\n"
     ]
    }
   ],
   "source": [
    "total_rewards = []\n",
    "step_idx = 0\n",
    "done_episodes = 0\n",
    "\n",
    "batch_episodes = 0\n",
    "batch_states, batch_actions, batch_qvals = [], [], []\n",
    "cur_rewards = []\n",
    "loss_rwd = 0.\n",
    "\n",
    "for step_idx, exp in enumerate(exp_source):\n",
    "    batch_states.append(exp.state)\n",
    "    batch_actions.append(int(exp.action))\n",
    "    x = torch.cat([float32_preprocessor(exp.state), float32_preprocessor([int(exp.action)])]).view(1, -1)\n",
    "    reward = reward_net(x)\n",
    "    cur_rewards.append(reward.item())\n",
    "\n",
    "    if exp.last_state is None:\n",
    "        batch_qvals.extend(calc_qvals(cur_rewards))\n",
    "        cur_rewards.clear()\n",
    "        batch_episodes += 1\n",
    "\n",
    "    new_rewards = exp_source.pop_total_rewards()\n",
    "    if new_rewards:\n",
    "        done_episodes += 1\n",
    "        reward = new_rewards[0]\n",
    "        total_rewards.append(reward)\n",
    "        mean_rewards = float(np.mean(total_rewards[-100:]))\n",
    "        writer.add_scalar('reward', reward, done_episodes)\n",
    "        writer.add_scalar('mean_reward', mean_rewards, done_episodes)\n",
    "        writer.add_scalar('loss_rwd', loss_rwd, done_episodes) \n",
    "        print(f'{step_idx}: reward: {reward:6.2f}, mean_100: {mean_rewards:6.2f}, '\n",
    "              f'episodes: {done_episodes}, reward function loss: {loss_rwd:6.4f}')\n",
    "        \n",
    "        if done_episodes%100 == 0 or mean_rewards>=500:\n",
    "            N_samp = 20\n",
    "            S1 = np.linspace(-5, 5, N_samp)\n",
    "            S2 = np.linspace(-3.1457, 3.1457, N_samp)\n",
    "            S3 = 0*S1\n",
    "            S4 = 0*S1\n",
    "            \n",
    "            S5 = np.ones(N_samp)\n",
    "            \n",
    "            for action in [0,1]:\n",
    "                Reward = np.zeros((N_samp,N_samp))\n",
    "                for i in range(N_samp):\n",
    "                    for j in range(N_samp):\n",
    "                        state = [S1[i], S2[j], 0, 0]\n",
    "#                         action = 1\n",
    "                        x = torch.cat([float32_preprocessor(state), float32_preprocessor([int(action)])]).view(1, -1)\n",
    "                        r = reward_net(x)\n",
    "                        Reward[i,j] = r\n",
    "\n",
    "                X, Y = np.meshgrid(S1, S2)\n",
    "                Z = Reward\n",
    "                fig = plt.figure()\n",
    "                ax = plt.axes(projection='3d')\n",
    "                ax.plot_surface(X, Y, Z, rstride=1, cstride=1,\n",
    "                                cmap='viridis', edgecolor='none')\n",
    "                ax.set_title('surface');\n",
    "                ax.set_xlabel('x')\n",
    "                ax.set_ylabel('y')\n",
    "                ax.set_zlabel('z');\n",
    "                ax.view_init(azim=90, elev=90)\n",
    "                if action ==0:\n",
    "                    writer.add_figure('x1 vs x2 with a = 0', fig, global_step=done_episodes/100)\n",
    "                if action ==1:\n",
    "                    writer.add_figure('x1 vs x2 with a = 1', fig, global_step=done_episodes/100)    \n",
    "            \n",
    "        if mean_rewards >= 500:\n",
    "            print(f'Solved in {step_idx} steps and {done_episodes} episodes!')\n",
    "            torch.save(agent_net.state_dict(), 'cartpole_learner.mod')\n",
    "            torch.save(reward_net.state_dict(), 'cartpole-v1_reward_func.mod')\n",
    "            break\n",
    "        \n",
    "\n",
    "    if batch_episodes < EPISODES_TO_TRAIN:\n",
    "        continue\n",
    "\n",
    "    states_v = torch.FloatTensor(batch_states)\n",
    "    batch_actions_t = torch.LongTensor(batch_actions)\n",
    "    batch_qvals_v = torch.FloatTensor(batch_qvals)\n",
    "\n",
    "    # reward function learning\n",
    "    demo_states = demonstrations['states']\n",
    "    demo_actions = demonstrations['actions']\n",
    "    demo_probs = demonstrations['traj_probs']\n",
    "    for rf_i in range(10):\n",
    "        selected = np.random.choice(len(demonstrations), DEMO_BATCH)\n",
    "        demo_states = demo_states[selected]\n",
    "        demo_actions = demo_actions[selected]\n",
    "        demo_probs = demo_probs[selected]\n",
    "        demo_batch_states, demo_batch_actions = [], []\n",
    "        for idx in range(len(demo_states)):\n",
    "            demo_batch_states.extend(demo_states[idx])\n",
    "            demo_batch_actions.extend(demo_actions[idx])\n",
    "        demo_batch_states = torch.FloatTensor(demo_batch_states)\n",
    "        demo_batch_actions = torch.FloatTensor(demo_batch_actions)\n",
    "        D_demo = torch.cat([demo_batch_states, demo_batch_actions.view(-1, 1)], dim=-1)\n",
    "        D_samp = torch.cat([states_v, batch_actions_t.float().view(-1, 1)], dim=-1)\n",
    "        D_samp = torch.cat([D_demo, D_samp])\n",
    "        # dummy importance weights - fix later\n",
    "        z = torch.ones((D_samp.shape[0], 1))\n",
    "\n",
    "        # objective\n",
    "        D_demo_out = reward_net(D_demo)\n",
    "        D_samp_out = reward_net(D_samp)\n",
    "        D_samp_out = z * torch.exp(D_samp_out)\n",
    "        loss_rwd = torch.mean(D_demo_out) - torch.log(torch.mean(D_samp_out))\n",
    "        loss_rwd = -loss_rwd  # for maximization\n",
    "\n",
    "        # update parameters\n",
    "        optimizer_reward.zero_grad()\n",
    "        loss_rwd.backward()\n",
    "        optimizer_reward.step()\n",
    "\n",
    "    # agent\n",
    "    optimizer_agent.zero_grad()\n",
    "    logits_v = agent_net(states_v)\n",
    "    log_prob_v = torch.log_softmax(logits_v, dim=1)\n",
    "    # REINFORCE\n",
    "    log_prob_actions_v = batch_qvals_v * log_prob_v[range(len(batch_states)), batch_actions_t]\n",
    "    loss_v = -log_prob_actions_v.mean()\n",
    "\n",
    "    loss_v.backward()\n",
    "    optimizer_agent.step()\n",
    "\n",
    "    batch_episodes = 0\n",
    "    batch_states.clear()\n",
    "    batch_actions.clear()\n",
    "    batch_qvals.clear()\n",
    "env.close()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial : 0  Reward:  10.0\n",
      "Trial : 1  Reward:  9.0\n",
      "Trial : 2  Reward:  11.0\n",
      "Trial : 3  Reward:  10.0\n",
      "Trial : 4  Reward:  15.0\n",
      "Trial : 5  Reward:  10.0\n",
      "Trial : 6  Reward:  10.0\n",
      "Trial : 7  Reward:  8.0\n",
      "Trial : 8  Reward:  8.0\n",
      "Trial : 9  Reward:  10.0\n"
     ]
    }
   ],
   "source": [
    "## Testing \n",
    "from util import Agent\n",
    "agent_net.eval()\n",
    "agent_ = Agent(agent_net, apply_softmax=True, preprocessor=ptan.agent.float32_preprocessor)\n",
    "\n",
    "for i in range(10):\n",
    "    state = env.reset()\n",
    "    Reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = agent_(state)\n",
    "        new_state, reward, done, _ = env.step(int(action))\n",
    "        Reward += reward\n",
    "    print(\"Trial :\", i, \" Reward: \", Reward)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04120645, -0.00432056, -0.01407001, -0.01187006])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib\n",
    "S1 = np.linspace(-5, 5, 100)\n",
    "S2 = np.linspace(-3.1457, 3.1457, 100)\n",
    "S3 = 0*S1\n",
    "S4 = 0*S1\n",
    "S5 = np.ones(100)\n",
    "Reward = np.zeros((100,100))\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        state = [S1[i], S2[j], 0, 0]\n",
    "        action = 1\n",
    "        x = torch.cat([float32_preprocessor(state), float32_preprocessor([int(action)])]).view(1, -1)\n",
    "        r = reward_net(x)\n",
    "        Reward[i,j] = r\n",
    "        \n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(X, Y, Z, rstride=1, cstride=1,\n",
    "                cmap='viridis', edgecolor='none')\n",
    "ax.set_title('surface');\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z');\n",
    "ax.view_init(azim=90, elev=90)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
